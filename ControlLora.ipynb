{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tunning diffsuion model using ControlLora"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yaIbLcf58C0A"
      },
      "source": [
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p5EXFaQznwu",
        "outputId": "8e3ca053-3669-4761-db03-758baea8bb92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' !pip install accelerate>=0.16.0\\n!pip install bitsandbytes>=0.37.0\\n!pip install datasets>=2.9.0\\n!pip install git+https://github.com/huggingface/diffusers\\n!pip install gradio>=3.18.0\\n!pip install opencv-python>=4.7.0\\n!pip install safetensors>=0.2.8\\n!pip install torch>=1.12.1\\n!pip install torchaudio>=0.12.1\\n!pip install torchvision>=0.13.1\\n!pip install transformers>=4.26.1\\n!pip install wandb>=0.13.1\\n!pip install ipywidgets>=7.7.1 '"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" !pip install accelerate>=0.16.0\n",
        "!pip install bitsandbytes>=0.37.0\n",
        "!pip install datasets>=2.9.0\n",
        "!pip install git+https://github.com/huggingface/diffusers\n",
        "!pip install gradio>=3.18.0\n",
        "!pip install opencv-python>=4.7.0\n",
        "!pip install safetensors>=0.2.8\n",
        "!pip install torch>=1.12.1\n",
        "!pip install torchaudio>=0.12.1\n",
        "!pip install torchvision>=0.13.1\n",
        "!pip install transformers>=4.26.1\n",
        "!pip install wandb>=0.13.1\n",
        "!pip install ipywidgets>=7.7.1 \"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tOAZUOYc8C0F"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W4ROS0zO8C0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import List, Tuple, Union\n",
        "from dataclasses import dataclass\n",
        "from diffusers.utils.outputs import BaseOutput\n",
        "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
        "from diffusers.models.modeling_utils import ModelMixin\n",
        "from diffusers.models.unet_2d_blocks import get_down_block as get_down_block_default\n",
        "from diffusers.models.resnet import Mish, Upsample2D, Downsample2D, upsample_2d, downsample_2d, partial\n",
        "from diffusers.models.cross_attention import CrossAttention # , LoRACrossAttnProcessor\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4kpeA6Rq8C0F"
      },
      "source": [
        "### Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IU8aHxyXz4sY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "class dataset_cls(data.Dataset):\n",
        "    DATASET_TYPE_DICT = {}\n",
        "\n",
        "    def __init__(self, tokenizer, resolution=512, use_crop=True, **kwargs):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def register_cls(cls, name: str):\n",
        "        dataset_cls.DATASET_TYPE_DICT['process/' + name] = cls\n",
        "\n",
        "    @staticmethod\n",
        "    def from_name(name: str):\n",
        "        dataset_cls: dataset_cls = dataset_cls.DATASET_TYPE_DICT[name]\n",
        "        return dataset_cls\n",
        "\n",
        "    @staticmethod\n",
        "    def control_channel():\n",
        "        return 3\n",
        "\n",
        "    @staticmethod\n",
        "    def cat_input(image: Image.Image, target: torch.Tensor, guide: torch.Tensor):\n",
        "        target = np.uint8(((target + 1) * 127.5)[0].permute(1,2,0).cpu().numpy().clip(0,255))\n",
        "        guide = np.uint8(((guide + 1) * 127.5)[0].permute(1,2,0).cpu().numpy().clip(0,255))\n",
        "        target = Image.fromarray(target).convert('RGB').resize(image.size)\n",
        "        guide = Image.fromarray(guide).convert('RGB').resize(image.size)\n",
        "        image_cat = Image.new('RGB', (image.size[0]*3,image.size[1]), (0,0,0))\n",
        "        image_cat.paste(target,(0,0))\n",
        "        image_cat.paste(guide,(image.size[0], 0))\n",
        "        image_cat.paste(image,(image.size[0]*2, 0))\n",
        "\n",
        "        return image_cat"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define conv blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ScpjFDbW8C0G"
      },
      "outputs": [],
      "source": [
        "class ConvBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_channels,\n",
        "        out_channels=None,\n",
        "        conv_kernel_size=3,\n",
        "        dropout=0.0,\n",
        "        temb_channels=512,\n",
        "        groups=32,\n",
        "        groups_out=None,\n",
        "        pre_norm=True,\n",
        "        eps=1e-6,\n",
        "        non_linearity=\"swish\",\n",
        "        time_embedding_norm=\"default\",\n",
        "        kernel=None,\n",
        "        output_scale_factor=1.0,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pre_norm = pre_norm\n",
        "        self.pre_norm = True\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.time_embedding_norm = time_embedding_norm\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.output_scale_factor = output_scale_factor\n",
        "\n",
        "        if groups_out is None:\n",
        "            groups_out = groups\n",
        "\n",
        "        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=conv_kernel_size, stride=1, padding=conv_kernel_size//2)\n",
        "\n",
        "        if temb_channels is not None:\n",
        "            if self.time_embedding_norm == \"default\":\n",
        "                time_emb_proj_out_channels = out_channels\n",
        "            elif self.time_embedding_norm == \"scale_shift\":\n",
        "                time_emb_proj_out_channels = out_channels * 2\n",
        "            else:\n",
        "                raise ValueError(f\"unknown time_embedding_norm : {self.time_embedding_norm} \")\n",
        "\n",
        "            self.time_emb_proj = torch.nn.Linear(temb_channels, time_emb_proj_out_channels)\n",
        "        else:\n",
        "            self.time_emb_proj = None\n",
        "\n",
        "        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        if non_linearity == \"swish\":\n",
        "            self.nonlinearity = lambda x: F.silu(x)\n",
        "        elif non_linearity == \"mish\":\n",
        "            self.nonlinearity = Mish()\n",
        "        elif non_linearity == \"silu\":\n",
        "            self.nonlinearity = nn.SiLU()\n",
        "\n",
        "        self.upsample = self.downsample = None\n",
        "        if self.up:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.upsample = lambda x: upsample_2d(x, kernel=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.upsample = partial(F.interpolate, scale_factor=2.0, mode=\"nearest\")\n",
        "            else:\n",
        "                self.upsample = Upsample2D(in_channels, use_conv=False)\n",
        "        elif self.down:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.downsample = lambda x: downsample_2d(x, kernel=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)\n",
        "            else:\n",
        "                self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name=\"op\")\n",
        "\n",
        "    def forward(self, input_tensor, temb):\n",
        "        hidden_states = input_tensor\n",
        "\n",
        "        hidden_states = self.norm1(hidden_states)\n",
        "        hidden_states = self.nonlinearity(hidden_states)\n",
        "\n",
        "        if self.upsample is not None:\n",
        "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
        "            if hidden_states.shape[0] >= 64:\n",
        "                input_tensor = input_tensor.contiguous()\n",
        "                hidden_states = hidden_states.contiguous()\n",
        "            input_tensor = self.upsample(input_tensor)\n",
        "            hidden_states = self.upsample(hidden_states)\n",
        "        elif self.downsample is not None:\n",
        "            input_tensor = self.downsample(input_tensor)\n",
        "            hidden_states = self.downsample(hidden_states)\n",
        "\n",
        "        hidden_states = self.conv1(hidden_states)\n",
        "\n",
        "        if temb is not None:\n",
        "            temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
        "\n",
        "        if temb is not None and self.time_embedding_norm == \"default\":\n",
        "            hidden_states = hidden_states + temb\n",
        "\n",
        "        hidden_states = self.norm2(hidden_states)\n",
        "\n",
        "        if temb is not None and self.time_embedding_norm == \"scale_shift\":\n",
        "            scale, shift = torch.chunk(temb, 2, dim=1)\n",
        "            hidden_states = hidden_states * (1 + scale) + shift\n",
        "\n",
        "        hidden_states = self.nonlinearity(hidden_states)\n",
        "\n",
        "        output_tensor = self.dropout(hidden_states)\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "class SimpleDownEncoderBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        dropout: float = 0.0,\n",
        "        num_layers: int = 1,\n",
        "        convnet_eps: float = 1e-6,\n",
        "        convnet_time_scale_shift: str = \"default\",\n",
        "        convnet_act_fn: str = \"swish\",\n",
        "        convnet_groups: int = 32,\n",
        "        convnet_pre_norm: bool = True,\n",
        "        convnet_kernel_size: int = 3,\n",
        "        output_scale_factor=1.0,\n",
        "        add_downsample=True,\n",
        "        downsample_padding=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        convnets = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            in_channels = in_channels if i == 0 else out_channels\n",
        "            convnets.append(\n",
        "                ConvBlock2D(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    temb_channels=None,\n",
        "                    eps=convnet_eps,\n",
        "                    groups=convnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=convnet_time_scale_shift,\n",
        "                    non_linearity=convnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=convnet_pre_norm,\n",
        "                    conv_kernel_size=convnet_kernel_size,\n",
        "                )\n",
        "            )\n",
        "        in_channels = in_channels if num_layers == 0 else out_channels\n",
        "\n",
        "        self.convnets = nn.ModuleList(convnets)\n",
        "\n",
        "        if add_downsample:\n",
        "            self.downsamplers = nn.ModuleList(\n",
        "                [\n",
        "                    Downsample2D(\n",
        "                        in_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            self.downsamplers = None\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for convnet in self.convnets:\n",
        "            hidden_states = convnet(hidden_states, temb=None)\n",
        "\n",
        "        if self.downsamplers is not None:\n",
        "            for downsampler in self.downsamplers:\n",
        "                hidden_states = downsampler(hidden_states)\n",
        "\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fv14u45e8C0H"
      },
      "outputs": [],
      "source": [
        "def get_down_block(\n",
        "    down_block_type,\n",
        "    num_layers,\n",
        "    in_channels,\n",
        "    out_channels,\n",
        "    temb_channels,\n",
        "    add_downsample,\n",
        "    resnet_eps,\n",
        "    resnet_act_fn,\n",
        "    attn_num_head_channels,\n",
        "    resnet_groups=None,\n",
        "    cross_attention_dim=None,\n",
        "    downsample_padding=None,\n",
        "    dual_cross_attention=False,\n",
        "    use_linear_projection=False,\n",
        "    only_cross_attention=False,\n",
        "    upcast_attention=False,\n",
        "    resnet_time_scale_shift=\"default\",\n",
        "    resnet_kernel_size=3,\n",
        "):\n",
        "    down_block_type = down_block_type[7:] if down_block_type.startswith(\"UNetRes\") else down_block_type\n",
        "    if down_block_type == \"SimpleDownEncoderBlock2D\":\n",
        "        return SimpleDownEncoderBlock2D(\n",
        "            num_layers=num_layers,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            add_downsample=add_downsample,\n",
        "            convnet_eps=resnet_eps,\n",
        "            convnet_act_fn=resnet_act_fn,\n",
        "            convnet_groups=resnet_groups,\n",
        "            downsample_padding=downsample_padding,\n",
        "            convnet_time_scale_shift=resnet_time_scale_shift,\n",
        "            convnet_kernel_size=resnet_kernel_size\n",
        "        )\n",
        "    else:\n",
        "        return get_down_block_default(\n",
        "            down_block_type,\n",
        "            num_layers,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            temb_channels,\n",
        "            add_downsample,\n",
        "            resnet_eps,\n",
        "            resnet_act_fn,\n",
        "            attn_num_head_channels,\n",
        "            resnet_groups=resnet_groups,\n",
        "            cross_attention_dim=cross_attention_dim,\n",
        "            downsample_padding=downsample_padding,\n",
        "            dual_cross_attention=dual_cross_attention,\n",
        "            use_linear_projection=use_linear_projection,\n",
        "            only_cross_attention=only_cross_attention,\n",
        "            upcast_attention=upcast_attention,\n",
        "            resnet_time_scale_shift=resnet_time_scale_shift,\n",
        "            # resnet_kernel_size=resnet_kernel_size\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define LoRA layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d7zzS-ET8C0H"
      },
      "outputs": [],
      "source": [
        "class LoRALinearLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank=4):\n",
        "        super().__init__()\n",
        "\n",
        "        if rank > min(in_features, out_features):\n",
        "            raise ValueError(f\"LoRA rank {rank} must be less or equal than {min(in_features, out_features)}\")\n",
        "\n",
        "        self.down = nn.Linear(in_features, rank, bias=False)\n",
        "        self.up = nn.Linear(rank, out_features, bias=False)\n",
        "\n",
        "        nn.init.normal_(self.down.weight, std=1 / rank)\n",
        "        nn.init.zeros_(self.up.weight)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        orig_dtype = hidden_states.dtype\n",
        "        dtype = self.down.weight.dtype\n",
        "\n",
        "        down_hidden_states = self.down(hidden_states.to(dtype))\n",
        "        up_hidden_states = self.up(down_hidden_states)\n",
        "\n",
        "        return up_hidden_states.to(orig_dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7qN9FpEe8C0I"
      },
      "outputs": [],
      "source": [
        "class LoRACrossAttnProcessor(nn.Module):\n",
        "    def __init__(\n",
        "            self, \n",
        "            hidden_size, \n",
        "            cross_attention_dim=None, \n",
        "            rank=4, \n",
        "            post_add=False,\n",
        "            key_states_skipped=False,\n",
        "            value_states_skipped=False,\n",
        "            output_states_skipped=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cross_attention_dim = cross_attention_dim\n",
        "        self.rank = rank\n",
        "        self.post_add = post_add\n",
        "\n",
        "        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n",
        "        if not key_states_skipped:\n",
        "            self.to_k_lora = LoRALinearLayer(\n",
        "                hidden_size if post_add else (cross_attention_dim or hidden_size), hidden_size, rank)\n",
        "        if not value_states_skipped:\n",
        "            self.to_v_lora = LoRALinearLayer(\n",
        "                hidden_size if post_add else (cross_attention_dim or hidden_size), hidden_size, rank)\n",
        "        if not output_states_skipped:\n",
        "            self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n",
        "\n",
        "        self.key_states_skipped: bool = key_states_skipped\n",
        "        self.value_states_skipped: bool = value_states_skipped\n",
        "        self.output_states_skipped: bool = output_states_skipped\n",
        "\n",
        "    def skip_key_states(self, is_skipped: bool = True):\n",
        "        if is_skipped == False:\n",
        "            assert hasattr(self, 'to_k_lora')\n",
        "        self.key_states_skipped = is_skipped\n",
        "\n",
        "    def skip_value_states(self, is_skipped: bool = True):\n",
        "        if is_skipped == False:\n",
        "            assert hasattr(self, 'to_q_lora')\n",
        "        self.value_states_skipped = is_skipped\n",
        "\n",
        "    def skip_output_states(self, is_skipped: bool = True):\n",
        "        if is_skipped == False:\n",
        "            assert hasattr(self, 'to_out_lora')\n",
        "        self.output_states_skipped = is_skipped\n",
        "\n",
        "    def __call__(\n",
        "        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n",
        "    ):\n",
        "        batch_size, sequence_length, _ = hidden_states.shape\n",
        "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
        "\n",
        "        query = attn.to_q(hidden_states) \n",
        "        query = query + scale * self.to_q_lora(query if self.post_add else hidden_states)\n",
        "        query = attn.head_to_batch_dim(query)\n",
        "\n",
        "        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n",
        "\n",
        "        key = attn.to_k(encoder_hidden_states) \n",
        "        if not self.key_states_skipped:\n",
        "            key = key + scale * self.to_k_lora(key if self.post_add else encoder_hidden_states)\n",
        "        value = attn.to_v(encoder_hidden_states)\n",
        "        if not self.value_states_skipped:\n",
        "            value = value + scale * self.to_v_lora(value if self.post_add else encoder_hidden_states)\n",
        "\n",
        "        key = attn.head_to_batch_dim(key)\n",
        "        value = attn.head_to_batch_dim(value)\n",
        "\n",
        "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
        "        hidden_states = torch.bmm(attention_probs, value)\n",
        "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
        "\n",
        "        # linear proj\n",
        "        out = attn.to_out[0](hidden_states)\n",
        "        if not self.output_states_skipped:\n",
        "            out = out + scale * self.to_out_lora(out if self.post_add else hidden_states)\n",
        "        hidden_states = out\n",
        "        # dropout\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OvMQ-0gZ8C0I"
      },
      "outputs": [],
      "source": [
        "class ControlLoRACrossAttnProcessor(LoRACrossAttnProcessor):\n",
        "    def __init__(\n",
        "            self, \n",
        "            hidden_size, \n",
        "            cross_attention_dim=None, \n",
        "            rank=4, \n",
        "            control_rank=None, \n",
        "            post_add=False, \n",
        "            concat_hidden=False,\n",
        "            control_channels=None,\n",
        "            control_self_add=True,\n",
        "            key_states_skipped=False,\n",
        "            value_states_skipped=False,\n",
        "            output_states_skipped=False,\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            hidden_size, \n",
        "            cross_attention_dim, \n",
        "            rank, \n",
        "            post_add=post_add,\n",
        "            key_states_skipped=key_states_skipped,\n",
        "            value_states_skipped=value_states_skipped,\n",
        "            output_states_skipped=output_states_skipped)\n",
        "\n",
        "        control_rank = rank if control_rank is None else control_rank\n",
        "        control_channels = hidden_size if control_channels is None else control_channels\n",
        "        self.concat_hidden = concat_hidden\n",
        "        self.control_self_add = control_self_add if control_channels is None else False\n",
        "        self.control_states: torch.Tensor = None\n",
        "\n",
        "        self.to_control = LoRALinearLayer(\n",
        "            control_channels + (hidden_size if concat_hidden else 0), \n",
        "            hidden_size, \n",
        "            control_rank)\n",
        "        self.pre_loras: List[LoRACrossAttnProcessor] = []\n",
        "        self.post_loras: List[LoRACrossAttnProcessor] = []\n",
        "\n",
        "    def inject_pre_lora(self, lora_layer):\n",
        "        self.pre_loras.append(lora_layer)\n",
        "    \n",
        "    def inject_post_lora(self, lora_layer):\n",
        "        self.post_loras.append(lora_layer)\n",
        "\n",
        "    def inject_control_states(self, control_states):\n",
        "        self.control_states = control_states\n",
        "\n",
        "    def process_control_states(self, hidden_states, scale=1.0):\n",
        "        control_states = self.control_states.to(hidden_states.dtype)\n",
        "        if hidden_states.ndim == 3 and control_states.ndim == 4:\n",
        "            batch, _, height, width = control_states.shape\n",
        "            control_states = control_states.permute(0, 2, 3, 1).reshape(batch, height * width, -1)\n",
        "            self.control_states = control_states\n",
        "        _control_states = control_states\n",
        "        if self.concat_hidden:\n",
        "            b1, b2 = control_states.shape[0], hidden_states.shape[0]\n",
        "            if b1 != b2:\n",
        "                control_states = control_states[:,None].repeat(1, b2//b1, *([1]*(len(control_states.shape)-1)))\n",
        "                control_states = control_states.view(-1, *control_states.shape[2:])\n",
        "            _control_states = torch.cat([hidden_states, control_states], -1)\n",
        "        _control_states = scale * self.to_control(_control_states)\n",
        "        if self.control_self_add:\n",
        "            control_states = control_states + _control_states\n",
        "        else:\n",
        "            control_states = _control_states\n",
        "\n",
        "        return control_states\n",
        "\n",
        "    def __call__(\n",
        "        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n",
        "    ):\n",
        "        pre_lora: LoRACrossAttnProcessor\n",
        "        post_lora: LoRACrossAttnProcessor\n",
        "        assert self.control_states is not None\n",
        "\n",
        "        batch_size, sequence_length, _ = hidden_states.shape\n",
        "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
        "        query = attn.to_q(hidden_states)\n",
        "        for pre_lora in self.pre_loras:\n",
        "            lora_in = query if pre_lora.post_add else hidden_states\n",
        "            if isinstance(pre_lora, ControlLoRACrossAttnProcessor):\n",
        "                lora_in = lora_in + pre_lora.process_control_states(hidden_states, scale)\n",
        "            query = query + scale * pre_lora.to_q_lora(lora_in)\n",
        "        query = query + scale * self.to_q_lora((\n",
        "            query if self.post_add else hidden_states) + self.process_control_states(hidden_states, scale))\n",
        "        for post_lora in self.post_loras:\n",
        "            lora_in = query if post_lora.post_add else hidden_states\n",
        "            if isinstance(post_lora, ControlLoRACrossAttnProcessor):\n",
        "                lora_in = lora_in + post_lora.process_control_states(hidden_states, scale)\n",
        "            query = query + scale * post_lora.to_q_lora(lora_in)\n",
        "        query = attn.head_to_batch_dim(query)\n",
        "\n",
        "        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n",
        "\n",
        "        key = attn.to_k(encoder_hidden_states)\n",
        "        for pre_lora in self.pre_loras:\n",
        "            if not pre_lora.key_states_skipped:\n",
        "                key = key + scale * pre_lora.to_k_lora(key if pre_lora.post_add else encoder_hidden_states)\n",
        "        if not self.key_states_skipped:\n",
        "            key = key + scale * self.to_k_lora(key if self.post_add else encoder_hidden_states)\n",
        "        for post_lora in self.post_loras:\n",
        "            if not post_lora.key_states_skipped:\n",
        "                key = key + scale * post_lora.to_k_lora(key if post_lora.post_add else encoder_hidden_states)\n",
        "        value = attn.to_v(encoder_hidden_states)\n",
        "        for pre_lora in self.pre_loras:\n",
        "            if not pre_lora.value_states_skipped:\n",
        "                value = value + pre_lora.to_v_lora(value if pre_lora.post_add else encoder_hidden_states)\n",
        "        if not self.value_states_skipped:\n",
        "            value = value + scale * self.to_v_lora(value if self.post_add else encoder_hidden_states)\n",
        "        for post_lora in self.post_loras:\n",
        "            if not post_lora.value_states_skipped:\n",
        "                value = value + post_lora.to_v_lora(value if post_lora.post_add else encoder_hidden_states)\n",
        "\n",
        "        key = attn.head_to_batch_dim(key)\n",
        "        value = attn.head_to_batch_dim(value)\n",
        "\n",
        "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
        "        hidden_states = torch.bmm(attention_probs, value)\n",
        "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
        "\n",
        "        # linear proj\n",
        "        out = attn.to_out[0](hidden_states)\n",
        "        for pre_lora in self.pre_loras:\n",
        "            if not pre_lora.output_states_skipped:\n",
        "                out = out + scale * pre_lora.to_out_lora(out if pre_lora.post_add else hidden_states)\n",
        "        out = out + scale * self.to_out_lora(out if self.post_add else hidden_states)\n",
        "        for post_lora in self.post_loras:\n",
        "            if not post_lora.output_states_skipped:\n",
        "                out = out + scale * post_lora.to_out_lora(out if post_lora.post_add else hidden_states)\n",
        "        hidden_states = out\n",
        "        # dropout\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "\n",
        "        return hidden_states"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ControlLora Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UqW701482-0Z"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ControlLoRAOutput(BaseOutput):\n",
        "    control_states: Tuple[torch.FloatTensor]\n",
        "\n",
        "\n",
        "class ControlLoRA(ModelMixin, ConfigMixin):\n",
        "    @register_to_config\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int = 3,\n",
        "        down_block_types: Tuple[str] = (\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "        ),\n",
        "        block_out_channels: Tuple[int] = (32, 64, 128, 256),\n",
        "        layers_per_block: int = 1,\n",
        "        act_fn: str = \"silu\",\n",
        "        norm_num_groups: int = 32,\n",
        "        lora_pre_down_block_types: Tuple[str] = (\n",
        "            None,\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "        ),\n",
        "        lora_pre_down_layers_per_block: int = 1,\n",
        "        lora_pre_conv_skipped: bool = False,\n",
        "        lora_pre_conv_types: Tuple[str] = (\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "            \"SimpleDownEncoderBlock2D\",\n",
        "        ),\n",
        "        lora_pre_conv_layers_per_block: int = 1,\n",
        "        lora_pre_conv_layers_kernel_size: int = 1,\n",
        "        lora_block_in_channels: Tuple[int] = (256, 256, 256, 256),\n",
        "        lora_block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n",
        "        lora_cross_attention_dims: Tuple[List[int]] = (\n",
        "            [None, 768, None, 768, None, 768, None, 768, None, 768], \n",
        "            [None, 768, None, 768, None, 768, None, 768, None, 768], \n",
        "            [None, 768, None, 768, None, 768, None, 768, None, 768], \n",
        "            [None, 768]\n",
        "        ),\n",
        "        lora_rank: int = 4,\n",
        "        lora_control_rank: int = None,\n",
        "        lora_post_add: bool = False,\n",
        "        lora_concat_hidden: bool = False,\n",
        "        lora_control_channels: Tuple[int] = (None, None, None, None),\n",
        "        lora_control_self_add: bool = True,\n",
        "        lora_key_states_skipped: bool = False,\n",
        "        lora_value_states_skipped: bool = False,\n",
        "        lora_output_states_skipped: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        lora_control_cls = ControlLoRACrossAttnProcessor\n",
        "\n",
        "        assert lora_block_in_channels[0] == block_out_channels[-1]\n",
        "        \n",
        "        if lora_pre_conv_skipped:\n",
        "            lora_control_channels = lora_block_in_channels\n",
        "            lora_control_self_add = False\n",
        "\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.lora_pre_down_layers_per_block = lora_pre_down_layers_per_block\n",
        "        self.lora_pre_conv_layers_per_block = lora_pre_conv_layers_per_block\n",
        "\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.down_blocks = nn.ModuleList([])\n",
        "        self.pre_lora_layers = nn.ModuleList([])\n",
        "        self.lora_layers = nn.ModuleList([])\n",
        "\n",
        "        # pre_down\n",
        "        pre_down_blocks = []\n",
        "        output_channel = block_out_channels[0]\n",
        "        for i, down_block_type in enumerate(down_block_types):\n",
        "            input_channel = output_channel\n",
        "            output_channel = block_out_channels[i]\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            pre_down_block = get_down_block(\n",
        "                down_block_type,\n",
        "                num_layers=self.layers_per_block,\n",
        "                in_channels=input_channel,\n",
        "                out_channels=output_channel,\n",
        "                add_downsample=not is_final_block,\n",
        "                resnet_eps=1e-6,\n",
        "                downsample_padding=0,\n",
        "                resnet_act_fn=act_fn,\n",
        "                resnet_groups=norm_num_groups,\n",
        "                attn_num_head_channels=None,\n",
        "                temb_channels=None,\n",
        "            )\n",
        "            pre_down_blocks.append(pre_down_block)\n",
        "        self.down_blocks.append(nn.Sequential(*pre_down_blocks))\n",
        "        self.pre_lora_layers.append(\n",
        "            get_down_block(\n",
        "                lora_pre_conv_types[0],\n",
        "                num_layers=self.lora_pre_conv_layers_per_block,\n",
        "                in_channels=lora_block_in_channels[0],\n",
        "                out_channels=(\n",
        "                    lora_block_out_channels[0] \n",
        "                    if lora_control_channels[0] is None \n",
        "                    else lora_control_channels[0]),\n",
        "                add_downsample=False,\n",
        "                resnet_eps=1e-6,\n",
        "                downsample_padding=0,\n",
        "                resnet_act_fn=act_fn,\n",
        "                resnet_groups=norm_num_groups,\n",
        "                attn_num_head_channels=None,\n",
        "                temb_channels=None,\n",
        "                resnet_kernel_size=lora_pre_conv_layers_kernel_size,\n",
        "            ) if not lora_pre_conv_skipped else nn.Identity()\n",
        "        )\n",
        "        self.lora_layers.append(\n",
        "            nn.ModuleList([\n",
        "                lora_control_cls(\n",
        "                    lora_block_out_channels[0], \n",
        "                    cross_attention_dim=cross_attention_dim, \n",
        "                    rank=lora_rank, \n",
        "                    control_rank=lora_control_rank,\n",
        "                    post_add=lora_post_add,\n",
        "                    concat_hidden=lora_concat_hidden,\n",
        "                    control_channels=lora_control_channels[0],\n",
        "                    control_self_add=lora_control_self_add,\n",
        "                    key_states_skipped=lora_key_states_skipped,\n",
        "                    value_states_skipped=lora_value_states_skipped,\n",
        "                    output_states_skipped=lora_output_states_skipped)\n",
        "                for cross_attention_dim in lora_cross_attention_dims[0]\n",
        "            ])\n",
        "        )\n",
        "        \n",
        "        # down\n",
        "        output_channel = lora_block_in_channels[0]\n",
        "        for i, down_block_type in enumerate(lora_pre_down_block_types):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            input_channel = output_channel\n",
        "            output_channel = lora_block_in_channels[i]\n",
        "\n",
        "            down_block = get_down_block(\n",
        "                down_block_type,\n",
        "                num_layers=self.lora_pre_down_layers_per_block,\n",
        "                in_channels=input_channel,\n",
        "                out_channels=output_channel,\n",
        "                add_downsample=True,\n",
        "                resnet_eps=1e-6,\n",
        "                downsample_padding=0,\n",
        "                resnet_act_fn=act_fn,\n",
        "                resnet_groups=norm_num_groups,\n",
        "                attn_num_head_channels=None,\n",
        "                temb_channels=None,\n",
        "            )\n",
        "            self.down_blocks.append(down_block)\n",
        "\n",
        "            self.pre_lora_layers.append(\n",
        "                get_down_block(\n",
        "                    lora_pre_conv_types[i],\n",
        "                    num_layers=self.lora_pre_conv_layers_per_block,\n",
        "                    in_channels=output_channel,\n",
        "                    out_channels=(\n",
        "                        lora_block_out_channels[i] \n",
        "                        if lora_control_channels[i] is None \n",
        "                        else lora_control_channels[i]),\n",
        "                    add_downsample=False,\n",
        "                    resnet_eps=1e-6,\n",
        "                    downsample_padding=0,\n",
        "                    resnet_act_fn=act_fn,\n",
        "                    resnet_groups=norm_num_groups,\n",
        "                    attn_num_head_channels=None,\n",
        "                    temb_channels=None,\n",
        "                    resnet_kernel_size=lora_pre_conv_layers_kernel_size,\n",
        "                ) if not lora_pre_conv_skipped else nn.Identity()\n",
        "            )\n",
        "            self.lora_layers.append(\n",
        "                nn.ModuleList([\n",
        "                    lora_control_cls(\n",
        "                        lora_block_out_channels[i], \n",
        "                        cross_attention_dim=cross_attention_dim, \n",
        "                        rank=lora_rank, \n",
        "                        control_rank=lora_control_rank,\n",
        "                        post_add=lora_post_add,\n",
        "                        concat_hidden=lora_concat_hidden,\n",
        "                        control_channels=lora_control_channels[i],\n",
        "                        control_self_add=lora_control_self_add,\n",
        "                        key_states_skipped=lora_key_states_skipped,\n",
        "                        value_states_skipped=lora_value_states_skipped,\n",
        "                        output_states_skipped=lora_output_states_skipped)\n",
        "                    for cross_attention_dim in lora_cross_attention_dims[i]\n",
        "                ])\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, return_dict: bool = True) -> Union[ControlLoRAOutput, Tuple]:\n",
        "        lora_layer: ControlLoRACrossAttnProcessor\n",
        "        \n",
        "        orig_dtype = x.dtype\n",
        "        dtype = self.conv_in.weight.dtype\n",
        "\n",
        "        h = x.to(dtype)\n",
        "        h = self.conv_in(h)\n",
        "        control_states_list = []\n",
        "\n",
        "        # down\n",
        "        for down_block, pre_lora_layer, lora_layer_list in zip(\n",
        "            self.down_blocks, self.pre_lora_layers, self.lora_layers):\n",
        "            h = down_block(h)\n",
        "            control_states = pre_lora_layer(h)\n",
        "            if isinstance(control_states, tuple):\n",
        "                control_states = control_states[0]\n",
        "            control_states = control_states.to(orig_dtype)\n",
        "            for lora_layer in lora_layer_list:\n",
        "                lora_layer.inject_control_states(control_states)\n",
        "            control_states_list.append(control_states)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(control_states_list)\n",
        "\n",
        "        return ControlLoRAOutput(control_states=tuple(control_states_list))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specifying Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s4uL7OSF3Bkw"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained_model_name_or_path,\n",
        "        revision=None,\n",
        "        dataset_name=None,\n",
        "        dataset_config_name=None,\n",
        "        train_data_dir=None,\n",
        "        image_column=\"image\",\n",
        "        guide_column=\"guide\",\n",
        "        caption_column=\"text\",\n",
        "        validation_prompt=None,\n",
        "        num_validation_images=4,\n",
        "        validation_epochs=1,\n",
        "        max_train_samples=None,\n",
        "        output_dir=\"sd-fill50k-model-control-lora\",\n",
        "        cache_dir=None,\n",
        "        seed=None,\n",
        "        resolution=512,\n",
        "        train_batch_size=16,\n",
        "        num_train_epochs=100,\n",
        "        max_train_steps=None,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=False,\n",
        "        learning_rate=1e-4,\n",
        "        scale_lr=False,\n",
        "        lr_scheduler=\"constant\",\n",
        "        lr_warmup_steps=500,\n",
        "        use_8bit_adam=False,\n",
        "        allow_tf32=False,\n",
        "        dataloader_num_workers=0,\n",
        "        adam_beta1=0.9,\n",
        "        adam_beta2=0.999,\n",
        "        adam_weight_decay=1e-2,\n",
        "        adam_epsilon=1e-08,\n",
        "        max_grad_norm=1.0,\n",
        "        push_to_hub=False,\n",
        "        hub_token=None,\n",
        "        hub_model_id=None,\n",
        "        logging_dir=\"logs\",\n",
        "        mixed_precision=None,\n",
        "        report_to=\"tensorboard\",\n",
        "        local_rank=-1,\n",
        "        checkpointing_steps=500,\n",
        "        resume_from_checkpoint=None,\n",
        "        enable_xformers_memory_efficient_attention=False,\n",
        "        control_lora_config=None,\n",
        "        wandb_project_name=None,\n",
        "    ):\n",
        "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
        "        self.revision = revision\n",
        "        self.dataset_name = dataset_name\n",
        "        self.dataset_config_name = dataset_config_name\n",
        "        self.train_data_dir = train_data_dir\n",
        "        self.image_column = image_column\n",
        "        self.guide_column = guide_column\n",
        "        self.caption_column = caption_column\n",
        "        self.validation_prompt = validation_prompt\n",
        "        self.num_validation_images = num_validation_images\n",
        "        self.validation_epochs = validation_epochs\n",
        "        self.max_train_samples = max_train_samples\n",
        "        self.output_dir = output_dir\n",
        "        self.cache_dir = cache_dir\n",
        "        self.seed = seed\n",
        "        self.resolution = resolution\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.num_train_epochs = num_train_epochs\n",
        "        self.max_train_steps = max_train_steps\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.gradient_checkpointing = gradient_checkpointing\n",
        "        self.learning_rate = learning_rate\n",
        "        self.scale_lr = scale_lr\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.lr_warmup_steps = lr_warmup_steps\n",
        "        self.use_8bit_adam = use_8bit_adam\n",
        "        self.allow_tf32 = allow_tf32\n",
        "        self.dataloader_num_workers = dataloader_num_workers\n",
        "        self.adam_beta1 = adam_beta1\n",
        "        self.adam_beta2 = adam_beta2\n",
        "        self.adam_weight_decay = adam_weight_decay\n",
        "        self.adam_epsilon = adam_epsilon\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.push_to_hub = push_to_hub\n",
        "        self.hub_token = hub_token\n",
        "        self.hub_model_id = hub_model_id\n",
        "        self.logging_dir = logging_dir\n",
        "        self.mixed_precision = mixed_precision\n",
        "        self.report_to = report_to\n",
        "        self.local_rank = local_rank\n",
        "        self.checkpointing_steps = checkpointing_steps\n",
        "        self.resume_from_checkpoint = resume_from_checkpoint\n",
        "        self.enable_xformers_memory_efficient_attention = enable_xformers_memory_efficient_attention\n",
        "        self.wandb_project_name = wandb_project_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"sd-unsplash_10k_blur_rand_KS-model-control-lora\"\n",
        "dataset_name = \"unsplash_10k_blur_rand_KS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Om74mf_a4sMo"
      },
      "outputs": [],
      "source": [
        "#@title Arguments\n",
        "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "revision = None #@param {type:\"string\"}\n",
        "dataset_name = \"wtcherr/\"+dataset_name #@param {type:\"string\"}\n",
        "dataset_config_name = None #@param {type:\"string\"}\n",
        "train_data_dir = None #@param {type:\"string\"}\n",
        "image_column = \"image\" #@param {type:\"string\"}\n",
        "guide_column = \"guide\" #@param {type:\"string\"}\n",
        "caption_column = \"text\" #@param {type:\"string\"}\n",
        "validation_prompt = \"a high-quality, detailed, and professional image\" #@param {type:\"string\"}\n",
        "num_validation_images = 3 #@param {type:\"integer\"}\n",
        "validation_epochs = 1 #@param {type:\"integer\"}\n",
        "max_train_samples = -1 #@param {type:\"integer\"}\n",
        "output_dir = \"ckpts/\"+model_name #@param {type:\"string\"}\n",
        "cache_dir = None #@param {type:\"string\"}\n",
        "seed = 42 #@param {type:\"integer\"}\n",
        "resolution = 512 #@param {type:\"integer\"}\n",
        "train_batch_size = 1 #@param {type:\"integer\"}\n",
        "num_train_epochs = 6 #@param {type:\"integer\"}\n",
        "max_train_steps = None #@param {type:\"string\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"integer\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "learning_rate = 1e-4 #@param {type:\"number\"}\n",
        "scale_lr = False #@param {type:\"boolean\"}\n",
        "lr_scheduler = \"constant\" #@param {type:\"string\"}\n",
        "lr_warmup_steps = 0 #@param {type:\"integer\"}\n",
        "use_8bit_adam = False #@param {type:\"boolean\"}\n",
        "allow_tf32 = False #@param {type:\"boolean\"}\n",
        "dataloader_num_workers = 0 #@param {type:\"integer\"}\n",
        "adam_beta1 = 0.9 #@param {type:\"number\"}\n",
        "adam_beta2 = 0.999 #@param {type:\"number\"}\n",
        "adam_weight_decay = 1e-2 #@param {type:\"number\"}\n",
        "adam_epsilon = 1e-08 #@param {type:\"number\"}\n",
        "max_grad_norm = 1.0 #@param {type:\"number\"}\n",
        "push_to_hub = True #@param {type:\"boolean\"}\n",
        "hub_token = \"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\" #@param {type:\"string\"}\n",
        "hub_model_id = None #@param {type:\"string\"}\n",
        "logging_dir = \"logs\" #@param {type:\"string\"}\n",
        "mixed_precision = \"fp16\" #@param {type:\"string\"}\n",
        "report_to = \"wandb\" #@param {type:\"string\"}\n",
        "local_rank = -1 #@param {type:\"integer\"}\n",
        "checkpointing_steps = 5000 #@param {type:\"integer\"}\n",
        "resume_from_checkpoint = \"latest\" #@param {type:\"string\"}\n",
        "enable_xformers_memory_efficient_attention = False #@param {type:\"boolean\"}\n",
        "wandb_project_name = \"ControlLora_unsplash_10k_blur_rand_KS\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b5Lk6wZI4uVX"
      },
      "outputs": [],
      "source": [
        "args = Args(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    revision=revision,\n",
        "    dataset_name=dataset_name,\n",
        "    dataset_config_name=dataset_config_name,\n",
        "    train_data_dir=train_data_dir,\n",
        "    image_column=image_column,\n",
        "    guide_column=guide_column,\n",
        "    caption_column=caption_column,\n",
        "    validation_prompt=validation_prompt,\n",
        "    num_validation_images=num_validation_images,\n",
        "    validation_epochs=validation_epochs,\n",
        "    max_train_samples=max_train_samples,\n",
        "    output_dir=output_dir,\n",
        "    cache_dir=cache_dir,\n",
        "    seed=seed,\n",
        "    resolution=resolution,\n",
        "    train_batch_size=train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    max_train_steps=max_train_steps,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    learning_rate=learning_rate,\n",
        "    scale_lr=scale_lr,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    lr_warmup_steps=lr_warmup_steps,\n",
        "    use_8bit_adam=use_8bit_adam,\n",
        "    allow_tf32=allow_tf32,\n",
        "    dataloader_num_workers=dataloader_num_workers,\n",
        "    adam_beta1=adam_beta1,\n",
        "    adam_beta2=adam_beta2,\n",
        "    adam_weight_decay=adam_weight_decay,\n",
        "    adam_epsilon=adam_epsilon,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    push_to_hub=push_to_hub,\n",
        "    hub_token=hub_token,\n",
        "    hub_model_id=hub_model_id,\n",
        "    logging_dir=logging_dir,\n",
        "    mixed_precision=mixed_precision,\n",
        "    report_to=report_to,\n",
        "    local_rank=local_rank,\n",
        "    checkpointing_steps=checkpointing_steps,\n",
        "    resume_from_checkpoint=resume_from_checkpoint,\n",
        "    enable_xformers_memory_efficient_attention=enable_xformers_memory_efficient_attention,\n",
        "    wandb_project_name=wandb_project_name\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ControlLoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fX-G6TDP4wBn"
      },
      "outputs": [],
      "source": [
        "control_lora_config = {\n",
        "  \"_class_name\": \"ControlLoRA\",\n",
        "  \"_diffusers_version\": \"0.13.0.dev0\",\n",
        "  \"act_fn\": \"silu\",\n",
        "  \"block_out_channels\": [\n",
        "    32,\n",
        "    64,\n",
        "    128,\n",
        "    256\n",
        "  ],\n",
        "  \"down_block_types\": [\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\"\n",
        "  ],\n",
        "  \"in_channels\": 3,\n",
        "  \"layers_per_block\": 1,\n",
        "  \"lora_block_in_channels\": [\n",
        "    256,\n",
        "    256,\n",
        "    256,\n",
        "    256\n",
        "  ],\n",
        "  \"lora_block_out_channels\": [\n",
        "    320,\n",
        "    640,\n",
        "    1280,\n",
        "    1280\n",
        "  ],\n",
        "  \"lora_control_rank\": None,\n",
        "  \"lora_cross_attention_dims\": [\n",
        "    [\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768\n",
        "    ],\n",
        "    [\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768\n",
        "    ],\n",
        "    [\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768,\n",
        "      None,\n",
        "      768\n",
        "    ],\n",
        "    [\n",
        "      None,\n",
        "      768\n",
        "    ]\n",
        "  ],\n",
        "  \"lora_post_add\": False,\n",
        "  \"lora_pre_conv_layers_kernel_size\": 1,\n",
        "  \"lora_pre_conv_layers_per_block\": 1,\n",
        "  \"lora_pre_conv_types\": [\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\"\n",
        "  ],\n",
        "  \"lora_pre_down_block_types\": [\n",
        "    None,\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\",\n",
        "    \"SimpleDownEncoderBlock2D\"\n",
        "  ],\n",
        "  \"lora_pre_down_layers_per_block\": 1,\n",
        "  \"lora_rank\": 4,\n",
        "  \"norm_num_groups\": 32\n",
        "}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "48Q7UmtG8C0K"
      },
      "outputs": [],
      "source": [
        "\"\"\"Fine-tuning script for Stable Diffusion for text2image with support for ControlLoRA.\"\"\"\n",
        "\"\"\"Code refer to https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py\"\"\"\n",
        "\n",
        "from diffusers import utils\n",
        "from diffusers.utils import deprecation_utils\n",
        "from diffusers.models import cross_attention\n",
        "utils.deprecate = lambda *arg, **kwargs: None\n",
        "deprecation_utils.deprecate = lambda *arg, **kwargs: None\n",
        "cross_attention.deprecate = lambda *arg, **kwargs: None\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import diffusers\n",
        "from diffusers import (\n",
        "    AutoencoderKL, \n",
        "    DDPMScheduler, \n",
        "    DPMSolverMultistepScheduler, \n",
        "    DiffusionPipeline, \n",
        "    UNet2DConditionModel)\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.utils import check_min_version, is_wandb_available\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "\n",
        "\n",
        "# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\n",
        "#check_min_version(\"0.13.0.dev0\")\n",
        "\n",
        "logger = get_logger(__name__, log_level=\"INFO\")\n",
        "\n",
        "\n",
        "def save_model_card(repo_name, images=None, base_model=str, dataset_name=str, repo_folder=None):\n",
        "    img_str = \"\"\n",
        "    for i, image in enumerate(images):\n",
        "        image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
        "        img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
        "\n",
        "    yaml = f\"\"\"\n",
        "---\n",
        "license: creativeml-openrail-m\n",
        "base_model: {base_model}\n",
        "tags:\n",
        "- stable-diffusion\n",
        "- stable-diffusion-diffusers\n",
        "- text-to-image\n",
        "- diffusers\n",
        "- lora\n",
        "- controlnet\n",
        "- control-lora\n",
        "inference: true\n",
        "---\n",
        "    \"\"\"\n",
        "    model_card = f\"\"\"\n",
        "# ControlLoRA text2image fine-tuning - {repo_name}\n",
        "These are ControlLoRA adaption weights for {base_model}. The weights were fine-tuned on the {dataset_name} dataset. You can find some example images in the following. \\n\n",
        "{img_str}\n",
        "\"\"\"\n",
        "    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n",
        "        f.write(yaml + model_card)\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
        "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
        "        args.local_rank = env_local_rank\n",
        "\n",
        "    # Sanity checks\n",
        "    if args.dataset_name is None and args.train_data_dir is None:\n",
        "        raise ValueError(\"Need either a dataset name or a training folder.\")\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n",
        "    if token is None:\n",
        "        token = HfFolder.get_token()\n",
        "    if organization is None:\n",
        "        username = whoami(token)[\"name\"]\n",
        "        return f\"{username}/{model_id}\"\n",
        "    else:\n",
        "        return f\"{organization}/{model_id}\"\n",
        "\n",
        "\n",
        "DATASET_NAME_MAPPING = (\"image\", \"guide\", \"text\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initializing the accelerator and logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M702h7L5AXGi",
        "outputId": "d1883637-45a9-4008-ab93-bb934a2f4a8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\accelerate\\accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
            "  warnings.warn(\n",
            "06/09/2023 00:40:00 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "d:\\BUE\\Graduation Project\\MyWork\\ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora is already a clone of https://huggingface.co/wtcherr/sd-unsplash_10k_blur_rand_KS-model-control-lora. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "06/09/2023 00:40:02 - WARNING - huggingface_hub.repository - d:\\BUE\\Graduation Project\\MyWork\\ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora is already a clone of https://huggingface.co/wtcherr/sd-unsplash_10k_blur_rand_KS-model-control-lora. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ],
      "source": [
        "args = parse_args()\n",
        "logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "    mixed_precision=args.mixed_precision,\n",
        "    log_with=args.report_to,\n",
        "    logging_dir=logging_dir,\n",
        ")\n",
        "if args.report_to == \"wandb\":\n",
        "    if not is_wandb_available():\n",
        "        raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n",
        "    import wandb\n",
        "\n",
        "# Make one log on every process with the configuration for debugging.\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger.info(accelerator.state, main_process_only=False)\n",
        "if accelerator.is_local_main_process:\n",
        "    datasets.utils.logging.set_verbosity_warning()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "    diffusers.utils.logging.set_verbosity_info()\n",
        "else:\n",
        "    datasets.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    diffusers.utils.logging.set_verbosity_error()\n",
        "\n",
        "# If passed along, set the training seed now.\n",
        "if args.seed is not None:\n",
        "    set_seed(args.seed)\n",
        "\n",
        "# Handle the repository creation\n",
        "if accelerator.is_main_process:\n",
        "    if args.push_to_hub:\n",
        "        if args.hub_model_id is None:\n",
        "            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
        "        else:\n",
        "            repo_name = args.hub_model_id\n",
        "        repo_name = create_repo(repo_name, exist_ok=True, token=args.hub_token)\n",
        "        repo = Repository(args.output_dir, clone_from=repo_name)\n",
        "\n",
        "        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
        "            if \"step_*\" not in gitignore:\n",
        "                gitignore.write(\"step_*\\n\")\n",
        "            if \"epoch_*\" not in gitignore:\n",
        "                gitignore.write(\"epoch_*\\n\")\n",
        "    elif args.output_dir is not None:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 404,
          "referenced_widgets": [
            "960ef00923314b44b48dbf0f93e55d1d",
            "2465fe7c0ab74a4080eb53dee24ff08f",
            "186e285d02b54d46a166196e68016370",
            "ad9f5a1961c74194a96fc2d45bae169e",
            "e153811fe24248ba86e7eab04d5b321c",
            "c769537ad1e04a0ab7b99f30a6f25ee1",
            "cbad2b67e8634e19935e7e1cd08e7132",
            "d045d38fa4294c8a862540ceee4d0f14",
            "1124a834994a46809923ea7bfca5ef99",
            "0bad5b4b98c64d4b896723b73557ecca",
            "dfbea5904be04ca193ab9a4dc185d7b0",
            "101740f9a77b4298a7fa099621b688b3",
            "1fbcde9e4199488bafb89314af9ecefa",
            "22730593a2354b7b8760451d001ce799",
            "78bc3bb5253d41cea61ed74c4dfc389c",
            "d5aa4122b3d3436ca2a7c1c39cc073ee",
            "e4bc916adac349f0855eee4698852842",
            "a7a47b3301614b64b1175402ac755a50",
            "aa9e118cef25475ebd2ddd602a7d544a",
            "1e402023ca5c4d8fa2f7567ec4265f04",
            "d611a42154a7467897d136fdec9dab5b",
            "0179ce57686942478141e238f20f561d",
            "2f01dfebdbeb40038cfa94955ce226c2",
            "7039c6c6d5964b4da4e78dcb1d67c7fa",
            "2c9b11386e8c4b2587a72f7d7c2908be",
            "d00149f7320144229512d878b634b374",
            "9c8bb3d428f946108b08270badfa8a1e",
            "5d482aac93994c3ea154ec09931c1dd1",
            "0bb3b70c9cee4b9f815e88e9f055310d",
            "db9eea5e03234df5b4e449f61adf2468",
            "8e304d882fd340bc80150bdd34ca73d3",
            "29c45d66b1a049a2b2c90ae5e5636b3b",
            "bb528303518c420c95d7a482f7de81b8",
            "b947856c025d498e86b529a57ba71d51",
            "f22b08607e294deeb886f8585366aba5",
            "9d49105631c04df3a3adbc9876c15a5d",
            "cf125cb518b84435996807786d06fbeb",
            "7d1337e958eb4ed4888055a7d1d1705e",
            "754c77edced14566a73bbe36e38a4f6f",
            "f27e05f702864c6eb29cfa8f4c00e94e",
            "1398519124704824af1304e7cb6fa24c",
            "35f2f25284df472fae4c0b5f942d3e11",
            "7762f808c01b4c6b8a345fda6cf640a7",
            "19af696c5ebc4826bffa4a2d1887cda5",
            "1ceb28ee3b5b4274a97d4b80cc28224d",
            "1b464bac943a4999918da4429fd18fb0",
            "f413de77aab54fa28b0e36f32431ee4d",
            "0f4dfbf821bd4f24a236ef4a689d6dd0",
            "25da4c5cfc1b4eaebb920ff66d760334",
            "f1783acd2a754f6199eb7a38ea5f4c79",
            "9784e490fba3435ebc460cdfdc09c72b",
            "e05b5e1dec334bd5853338e53919384e",
            "759430d292bf4e47954abc0809be3448",
            "3c4f6d4791d34ee998677ac7c632122d",
            "7f116b4277744d7683f3cc1aa11cc3c2",
            "2acf111eaec243a9b909b563a8a648fc",
            "ae35ff01b9f44873a9d1d5014c6889c1",
            "bffaee3eea6d49cda76573c7f2eef3d7",
            "93dbee95832e416ca7d6529349217d37",
            "50c901f0f12e466d9bd5e373ce950b3b",
            "401b536905a74447a33419b60a42579f",
            "0584eff53faa4c9ea756cef809c1f0a3",
            "b6c835d2d19540378d038cdd998bcccd",
            "be767ce1c0b0435db7402b6667526565",
            "7fd37bf306d54f8eb7079aed5a97f070",
            "98b76fc606ee4340808f0ce5f9a6489f",
            "a195f39609cf4fbc8cd43325f5db9e65",
            "e7961fc269e642b68399ce2aacc3eb6e",
            "995370a5f3bf4ba1aa4a9b8abbe1836b",
            "26209a2b0f834e569b73a90252519426",
            "ef2b1e23571f4ff282935a444df76305",
            "d63a58545afa48a1b3e526ff5a28db59",
            "404ecdac1e4a4396a161fc182f60bf5c",
            "ab900241335744eeb1bd0dbfea25e6dc",
            "678c8326ec4e4e8e87a2aabc4530c220",
            "d034faafcf2e48cf9d81b88688f87f99",
            "a1d87067b5c248898a88acb646351d8d",
            "4cc5a68581024c30a4aa61376f5e5180",
            "e44a695a19794f74b7c943968e460388",
            "d0d1b5fe7c234d2faa122d807b1b3e38",
            "cc975b6d922b4c78871a9a6a7dbf17cc",
            "639b5747bb11424fb902b64b6b5fa1f9",
            "df178eee77c5401ebdba2caa73f3a458",
            "55a5e6cfc187405699e934063cee7ff1",
            "427c77b11642447e9e47869d425fb024",
            "5f280ce78ebf413a889d868e3f6aa54b",
            "13c6f50c9fd343f482eda0f335d4a5e2",
            "df5128cdfe954ddca98544cf8f909d66",
            "46052f71dac44e238aaba9106b6c72e9",
            "ae72a057a76741fb856574c150816d47",
            "06cdcacda0ed440dbd95df889752576a",
            "ff2cd42d9558444aa16a6e3e74b9e6b0",
            "e8ce7be799de41f4a296d84659b8a938",
            "038b92205454452e9fee0406fba28a7b",
            "7ba1c9ef340a47559ed70007723d0655",
            "08a272c3046a4d78867c7254edaa6a9e",
            "be22951c14034ccf9721def748f0418d",
            "8bcf5a68d29344c381d671bdc2abc970",
            "df61a047c3ae4548b519556bc95a2b07",
            "43dac5ff93094eea8568417e0ece935c",
            "b8f84b4a31fd4a278576996b35ce554c",
            "e47008f6e3f04cee91176c149432ced1",
            "6c55328b46874580901c8f4da02b833b",
            "d754221f7b424f1da317c24ed29e6e94",
            "9b5b6f1d5af84265b2454d8e9cfbcb46",
            "20754b37e94b4fafacd08629f7eeedc6",
            "e59673d05ce14c3b8965059f82757438",
            "ae6009824cec4d85ad1f3ab93bbd88aa",
            "5deddf6e7ec04837ba60fec2ba324821",
            "c32b04b7a881493bb9a15885949c9f8c",
            "520af4d76b8d4e6488dc3ccca3d4a1e8",
            "83d70d17163f42bfb05f23538af4d1ee",
            "4f969668494d4f9e8b7b8f896740b719",
            "9cbf2fb66927487e93bc25b37721cd9a",
            "d14e6af6bdf54ddb91247ac5e2dfa549",
            "e6f3b83a4af34acca88be165acdff131",
            "65df24c7d83f4b6792fb39aa6d844c02",
            "8ebf44208a4d4607a0a5f4b4be821bed",
            "39e9a0fbea4044cdad024e6d4f5edbbc",
            "378befb8b73c45a98848754a589f97a4",
            "076e52d984094285ba1fa7bc1a6da80b"
          ]
        },
        "id": "SIoVxVlE8C0K",
        "outputId": "217093bb-d6d4-438d-da6a-f1f1b8514fae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'prediction_type', 'sample_max_value', 'thresholding', 'clip_sample_range', 'dynamic_thresholding_ratio', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "{'projection_class_embeddings_input_dim', 'class_embeddings_concat', 'conv_in_kernel', 'cross_attention_norm', 'encoder_hid_dim', 'conv_out_kernel', 'class_embed_type', 'resnet_time_scale_shift', 'time_embedding_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_cond_proj_dim', 'mid_block_type', 'addition_embed_type_num_heads', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'num_class_embeds', 'time_embedding_type', 'addition_embed_type', 'resnet_skip_time_act', 'use_linear_projection', 'only_cross_attention', 'upcast_attention', 'timestep_post_act'} was not found in config. Values will be initialized to default values.\n"
          ]
        }
      ],
      "source": [
        "# Load scheduler, tokenizer and models.\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n",
        ")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n",
        ")\n",
        "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E1YU3hFK8C0K",
        "outputId": "77ad4e20-f7e9-4a9f-edf7-77f774326e72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet2DConditionModel(\n",
            "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (time_proj): Timesteps()\n",
            "  (time_embedding): TimestepEmbedding(\n",
            "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
            "    (act): SiLU()\n",
            "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "  )\n",
            "  (down_blocks): ModuleList(\n",
            "    (0): CrossAttnDownBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-1): 2 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0-1): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "      (downsamplers): ModuleList(\n",
            "        (0): Downsample2D(\n",
            "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): CrossAttnDownBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-1): 2 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "      (downsamplers): ModuleList(\n",
            "        (0): Downsample2D(\n",
            "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): CrossAttnDownBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-1): 2 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "      (downsamplers): ModuleList(\n",
            "        (0): Downsample2D(\n",
            "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): DownBlock2D(\n",
            "      (resnets): ModuleList(\n",
            "        (0-1): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (up_blocks): ModuleList(\n",
            "    (0): UpBlock2D(\n",
            "      (resnets): ModuleList(\n",
            "        (0-2): 3 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (upsamplers): ModuleList(\n",
            "        (0): Upsample2D(\n",
            "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): CrossAttnUpBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-2): 3 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0-1): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (upsamplers): ModuleList(\n",
            "        (0): Upsample2D(\n",
            "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): CrossAttnUpBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-2): 3 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (upsamplers): ModuleList(\n",
            "        (0): Upsample2D(\n",
            "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): CrossAttnUpBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-2): 3 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1-2): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mid_block): UNetMidBlock2DCrossAttn(\n",
            "    (attentions): ModuleList(\n",
            "      (0): Transformer2DModel(\n",
            "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (transformer_blocks): ModuleList(\n",
            "          (0): BasicTransformerBlock(\n",
            "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn1): Attention(\n",
            "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_out): ModuleList(\n",
            "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn2): Attention(\n",
            "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
            "              (to_out): ModuleList(\n",
            "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "            (ff): FeedForward(\n",
            "              (net): ModuleList(\n",
            "                (0): GEGLU(\n",
            "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                )\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (resnets): ModuleList(\n",
            "      (0-1): 2 x ResnetBlock2D(\n",
            "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (nonlinearity): SiLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "  (conv_act): SiLU()\n",
            "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(unet)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Injecting ControlLoRA for cross-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8MjbjaPe8C0L",
        "outputId": "ba80738f-3337-4d55-8f42-4baf4f35057c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{'lora_key_states_skipped', 'lora_control_self_add', 'lora_output_states_skipped', 'lora_value_states_skipped', 'lora_pre_conv_skipped', 'lora_control_channels', 'lora_concat_hidden'} was not found in config. Values will be initialized to default values.\n"
          ]
        }
      ],
      "source": [
        "n_ch = len(unet.config.block_out_channels)\n",
        "control_ids = [i for i in range(n_ch)]\n",
        "cross_attention_dims = {i: [] for i in range(n_ch)}\n",
        "for name in unet.attn_processors.keys():\n",
        "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "    if name.startswith(\"mid_block\"):\n",
        "        control_id = control_ids[-1]\n",
        "    elif name.startswith(\"up_blocks\"):\n",
        "        block_id = int(name[len(\"up_blocks.\")])\n",
        "        control_id = list(reversed(control_ids))[block_id]\n",
        "    elif name.startswith(\"down_blocks\"):\n",
        "        block_id = int(name[len(\"down_blocks.\")])\n",
        "        control_id = control_ids[block_id]\n",
        "    cross_attention_dims[control_id].append(cross_attention_dim)\n",
        "cross_attention_dims = tuple([cross_attention_dims[control_id] for control_id in control_ids])\n",
        "\n",
        "control_lora = ControlLoRA.from_config(control_lora_config)\n",
        "\n",
        "# freeze parameters of models to save more memory\n",
        "unet.requires_grad_(False)\n",
        "vae.requires_grad_(False)\n",
        "\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
        "# as these models are only used for inference, keeping weights in full precision is not required.\n",
        "weight_dtype = torch.float32\n",
        "if accelerator.mixed_precision == \"fp16\":\n",
        "    weight_dtype = torch.float16\n",
        "elif accelerator.mixed_precision == \"bf16\":\n",
        "    weight_dtype = torch.bfloat16\n",
        "\n",
        "# Move unet, vae and text_encoder to device and cast to weight_dtype\n",
        "unet.to(accelerator.device, dtype=weight_dtype)\n",
        "vae.to(accelerator.device, dtype=weight_dtype)\n",
        "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "control_lora.to(accelerator.device) # control_lora.to(accelerator.device), dtype=weight_dtype)\n",
        "\n",
        "if args.enable_xformers_memory_efficient_attention:\n",
        "    if is_xformers_available():\n",
        "        unet.enable_xformers_memory_efficient_attention()\n",
        "    else:\n",
        "        raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
        "\n",
        "# now we will add new LoRA weights to the attention layers\n",
        "# It's important to realize here how many attention weights will be added and of which sizes\n",
        "# The sizes of the attention layers consist only of two different variables:\n",
        "# 1) - the \"hidden_size\", which is increased according to `unet.config.block_out_channels`.\n",
        "# 2) - the \"cross attention size\", which is set to `unet.config.cross_attention_dim`.\n",
        "\n",
        "# Let's first see how many attention processors we will have to set.\n",
        "# For Stable Diffusion, it should be equal to:\n",
        "# - down blocks (2x attention layers) * (2x transformer layers) * (3x down blocks) = 12\n",
        "# - mid blocks (2x attention layers) * (1x transformer layers) * (1x mid blocks) = 2\n",
        "# - up blocks (2x attention layers) * (3x transformer layers) * (3x up blocks) = 18\n",
        "# => 32 layers\n",
        "\n",
        "# Set correct lora layers\n",
        "lora_attn_procs = {}\n",
        "lora_layers_list = list([list(layer_list) for layer_list in control_lora.lora_layers])\n",
        "for name in unet.attn_processors.keys():\n",
        "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "    if name.startswith(\"mid_block\"):\n",
        "        control_id = control_ids[-1]\n",
        "    elif name.startswith(\"up_blocks\"):\n",
        "        block_id = int(name[len(\"up_blocks.\")])\n",
        "        control_id = list(reversed(control_ids))[block_id]\n",
        "    elif name.startswith(\"down_blocks\"):\n",
        "        block_id = int(name[len(\"down_blocks.\")])\n",
        "        control_id = control_ids[block_id]\n",
        "\n",
        "    lora_layers = lora_layers_list[control_id]\n",
        "    if len(lora_layers) != 0:\n",
        "        lora_layer = lora_layers.pop(0) \n",
        "        lora_attn_procs[name] = lora_layer\n",
        "\n",
        "unet.set_attn_processor(lora_attn_procs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FZJu5Ie2xJOv",
        "outputId": "5d7d64c9-a960-4b44-f76e-abfe0d50db82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNet2DConditionModel(\n",
            "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (time_proj): Timesteps()\n",
            "  (time_embedding): TimestepEmbedding(\n",
            "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
            "    (act): SiLU()\n",
            "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "  )\n",
            "  (down_blocks): ModuleList(\n",
            "    (0): CrossAttnDownBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-1): 2 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0-1): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "      (downsamplers): ModuleList(\n",
            "        (0): Downsample2D(\n",
            "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): CrossAttnDownBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-1): 2 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "      (downsamplers): ModuleList(\n",
            "        (0): Downsample2D(\n",
            "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): CrossAttnDownBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-1): 2 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "      (downsamplers): ModuleList(\n",
            "        (0): Downsample2D(\n",
            "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): DownBlock2D(\n",
            "      (resnets): ModuleList(\n",
            "        (0-1): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (up_blocks): ModuleList(\n",
            "    (0): UpBlock2D(\n",
            "      (resnets): ModuleList(\n",
            "        (0-2): 3 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (upsamplers): ModuleList(\n",
            "        (0): Upsample2D(\n",
            "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): CrossAttnUpBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-2): 3 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0-1): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (upsamplers): ModuleList(\n",
            "        (0): Upsample2D(\n",
            "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): CrossAttnUpBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-2): 3 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=640, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=640, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
            "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (upsamplers): ModuleList(\n",
            "        (0): Upsample2D(\n",
            "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): CrossAttnUpBlock2D(\n",
            "      (attentions): ModuleList(\n",
            "        (0-2): 3 x Transformer2DModel(\n",
            "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
            "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (transformer_blocks): ModuleList(\n",
            "            (0): BasicTransformerBlock(\n",
            "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn1): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (attn2): Attention(\n",
            "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
            "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
            "                (to_out): ModuleList(\n",
            "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                )\n",
            "                (processor): ControlLoRACrossAttnProcessor(\n",
            "                  (to_q_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_k_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_v_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_out_lora): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                  (to_control): LoRALinearLayer(\n",
            "                    (down): Linear(in_features=320, out_features=4, bias=False)\n",
            "                    (up): Linear(in_features=4, out_features=320, bias=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
            "              (ff): FeedForward(\n",
            "                (net): ModuleList(\n",
            "                  (0): GEGLU(\n",
            "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
            "                  )\n",
            "                  (1): Dropout(p=0.0, inplace=False)\n",
            "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (resnets): ModuleList(\n",
            "        (0): ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1-2): 2 x ResnetBlock2D(\n",
            "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
            "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
            "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (nonlinearity): SiLU()\n",
            "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mid_block): UNetMidBlock2DCrossAttn(\n",
            "    (attentions): ModuleList(\n",
            "      (0): Transformer2DModel(\n",
            "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
            "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (transformer_blocks): ModuleList(\n",
            "          (0): BasicTransformerBlock(\n",
            "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn1): Attention(\n",
            "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_out): ModuleList(\n",
            "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (processor): ControlLoRACrossAttnProcessor(\n",
            "                (to_q_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_k_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_v_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_out_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_control): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn2): Attention(\n",
            "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
            "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
            "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
            "              (to_out): ModuleList(\n",
            "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (processor): ControlLoRACrossAttnProcessor(\n",
            "                (to_q_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_k_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_v_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=768, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_out_lora): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "                (to_control): LoRALinearLayer(\n",
            "                  (down): Linear(in_features=1280, out_features=4, bias=False)\n",
            "                  (up): Linear(in_features=4, out_features=1280, bias=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
            "            (ff): FeedForward(\n",
            "              (net): ModuleList(\n",
            "                (0): GEGLU(\n",
            "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
            "                )\n",
            "                (1): Dropout(p=0.0, inplace=False)\n",
            "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (resnets): ModuleList(\n",
            "      (0-1): 2 x ResnetBlock2D(\n",
            "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (nonlinearity): SiLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
            "  (conv_act): SiLU()\n",
            "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1C-lJE6Y8C0L",
        "outputId": "f1f6e7b0-549b-493c-859e-f068ab746a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 996864 || all params: 1067232171 || trainable%: 0.0934064795915902\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(models):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for model in models:\n",
        "        for _, param in model.named_parameters():\n",
        "            all_param += param.numel()\n",
        "            if param.requires_grad:\n",
        "                trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "  \n",
        "print_trainable_parameters([text_encoder, vae, unet])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bBQgTK1S8C0L"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48efa8132b234935956eef73d5813d08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/447 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/09/2023 00:40:25 - WARNING - datasets.builder - Found cached dataset parquet (C:/Users/Khaled/.cache/huggingface/datasets/wtcherr___parquet/wtcherr--unsplash_10k_blur_rand_KS-3297086daeec25a3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e53a085de3d545dfaea2655a6f279fb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Enable TF32 for faster training on Ampere GPUs,\n",
        "# cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "if args.allow_tf32:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "if args.scale_lr:\n",
        "    args.learning_rate = (\n",
        "        args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
        "    )\n",
        "\n",
        "# Initialize the optimizer\n",
        "if args.use_8bit_adam:\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "    except ImportError:\n",
        "        raise ImportError(\n",
        "            \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
        "        )\n",
        "\n",
        "    optimizer_cls = bnb.optim.AdamW8bit\n",
        "else:\n",
        "    optimizer_cls = torch.optim.AdamW\n",
        "\n",
        "optimizer = optimizer_cls(\n",
        "    control_lora.parameters(),\n",
        "    lr=args.learning_rate,\n",
        "    betas=(args.adam_beta1, args.adam_beta2),\n",
        "    weight_decay=args.adam_weight_decay,\n",
        "    eps=args.adam_epsilon,\n",
        ")\n",
        "\n",
        "# Preprocessing the datasets.\n",
        "# We need to tokenize input captions and transform the images.\n",
        "def tokenize_captions(examples, is_train=True):\n",
        "    captions = []\n",
        "    for caption in examples[caption_column]:\n",
        "        if isinstance(caption, str):\n",
        "            captions.append(caption)\n",
        "        elif isinstance(caption, (list, np.ndarray)):\n",
        "            # take a random caption if there are multiple\n",
        "            captions.append(random.choice(caption) if is_train else caption[0])\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
        "            )\n",
        "    inputs = tokenizer(\n",
        "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return inputs.input_ids\n",
        "\n",
        "# Get the datasets: you can either provide your own training and evaluation files (see below)\n",
        "# or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
        "\n",
        "# In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
        "# download the dataset.\n",
        "dataset_cls = dataset_cls\n",
        "use_custom_dataset = False\n",
        "if args.dataset_name.startswith('process/'):\n",
        "    # Use custom dataset define in process\n",
        "    use_custom_dataset = True\n",
        "    dataset_cls = dataset_cls.from_name(args.dataset_name)\n",
        "    dataset = dataset_cls(tokenize_captions, resolution=args.resolution, use_crop=True)\n",
        "elif args.dataset_name is not None:\n",
        "    # Downloading and loading a dataset from the hub.\n",
        "    dataset = load_dataset(\n",
        "        args.dataset_name,\n",
        "        args.dataset_config_name,\n",
        "        cache_dir=args.cache_dir,\n",
        "    )\n",
        "else:\n",
        "    data_files = {}\n",
        "    if args.train_data_dir is not None:\n",
        "        data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n",
        "    dataset = load_dataset(\n",
        "        \"imagefolder\",\n",
        "        data_files=data_files,\n",
        "        cache_dir=args.cache_dir,\n",
        "    )\n",
        "    # See more about loading custom images at\n",
        "    # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n",
        "\n",
        "if use_custom_dataset:\n",
        "    collate_fn = None\n",
        "    train_dataset = dataset\n",
        "    caption_column = args.caption_column\n",
        "else:\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    column_names = dataset[\"train\"].column_names\n",
        "\n",
        "    # 6. Get the column names for input/target.\n",
        "    dataset_columns = DATASET_NAME_MAPPING\n",
        "    if args.image_column is None:\n",
        "        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
        "    else:\n",
        "        image_column = args.image_column\n",
        "        if image_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if args.guide_column is None:\n",
        "        guide_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
        "    else:\n",
        "        guide_column = args.guide_column\n",
        "        if guide_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--guide_column' value '{args.guide_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "    if args.caption_column is None:\n",
        "        caption_column = dataset_columns[2] if dataset_columns is not None else column_names[2]\n",
        "    else:\n",
        "        caption_column = args.caption_column\n",
        "        if caption_column not in column_names:\n",
        "            raise ValueError(\n",
        "                f\"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    train_transforms = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def preprocess_train(examples):\n",
        "        images, guides = [], []\n",
        "        for image, guide in zip(examples[image_column], examples[guide_column]):\n",
        "            image, guide = image.convert(\"RGB\"), guide.convert(\"RGB\")\n",
        "            image, guide = train_transforms(image), train_transforms(guide)\n",
        "            c, h, w = image.shape\n",
        "            y1, x1 = 0, 0\n",
        "            if h != args.resolution:\n",
        "                y1 = torch.randint(0, h - args.resolution, (1, )).item()\n",
        "            elif w != args.resolution:\n",
        "                x1 = torch.randint(0, w - args.resolution, (1, )).item()\n",
        "            y2, x2 = y1 + args.resolution, x1 + args.resolution\n",
        "            image = image[:,y1:y2,x1:x2]\n",
        "            guide = guide[:,y1:y2,x1:x2]\n",
        "            images.append(image)\n",
        "            guides.append(guide)\n",
        "            \n",
        "        examples[\"pixel_values\"] = images\n",
        "        examples[\"guide_values\"] = guides\n",
        "        examples[\"input_ids\"] = tokenize_captions(examples)\n",
        "        return examples\n",
        "\n",
        "    with accelerator.main_process_first():\n",
        "        if args.max_train_samples != -1 and args.max_train_samples != None:\n",
        "            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n",
        "        # Set the training transforms\n",
        "        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "        guide_values = torch.stack([example[\"guide_values\"] for example in examples])\n",
        "        guide_values = guide_values.to(memory_format=torch.contiguous_format).float()\n",
        "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "        return {\"pixel_values\": pixel_values, \"guide_values\": guide_values, \"input_ids\": input_ids}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "49CDLIHy8C0N"
      },
      "outputs": [],
      "source": [
        "# DataLoaders creation:\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=args.train_batch_size,\n",
        "    num_workers=args.dataloader_num_workers,\n",
        ")\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=1,\n",
        "    num_workers=0,\n",
        ")\n",
        "val_iter = iter(val_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sTb4L8FL8C0N"
      },
      "outputs": [],
      "source": [
        "# Scheduler and math around the number of training steps.\n",
        "overrode_max_train_steps = False\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "if args.max_train_steps is None:\n",
        "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    overrode_max_train_steps = True\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    args.lr_scheduler,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
        "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aDrA8soM8C0N"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/09/2023 00:40:31 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwtcherr\u001b[0m (\u001b[33mwtcherrr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>d:\\BUE\\Graduation Project\\MyWork\\wandb\\run-20230609_004035-maqyhey0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS/runs/maqyhey0' target=\"_blank\">vocal-moon-2</a></strong> to <a href='https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS' target=\"_blank\">https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS/runs/maqyhey0' target=\"_blank\">https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS/runs/maqyhey0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Prepare everything with our `accelerator`.\n",
        "control_lora, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    control_lora, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "if overrode_max_train_steps:\n",
        "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "# Afterwards we recalculate our number of training epochs\n",
        "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "# We need to initialize the trackers we use, and also store our configuration.\n",
        "# The trackers initializes automatically on the main process.\n",
        "if accelerator.is_main_process:\n",
        "    accelerator.init_trackers(args.wandb_project_name, config=vars(args))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 599,
          "referenced_widgets": [
            "94e676b502294be49e0e1d6aa8a7cbae",
            "c610bc21a18347d3b1ad203ea5836404",
            "438f7f182ab847eb90d863be7ee3d4eb",
            "730cd649635745548aa13f347440c967",
            "9c51968607594b7da90149c8c2f70501",
            "6d4fb39ad1ab4393bc7d364104c06787",
            "5804ae1eb2a845ce9cee1e5f1b8cf795",
            "3f25a9cb320e4835ad45b31f19c3ff79",
            "b6da99b1947042fb98dd567356eb7c99",
            "b872d111a4134886b3039b7b0cc48f98",
            "c1b2c68968ba4d1982af643c00e652a4",
            "7343b4f18cdf4b76aa1a941f6ab9caa6",
            "853ee94ef8234dd1a4de51f4b5f244b0",
            "811ebb183ae640d4a009a7894fc7250b",
            "1f6ebe644358489f9042a601d5cbce2a",
            "c83d5c1515d441c79c7e512735124eb3",
            "b884e3d925244864ba21221ffe01a0f0",
            "13b4f517e64844aabcb7842dd42fd228",
            "b545febebee44dd49c96ee303c02529e",
            "cb0d8321e4e74731b2708009939c3d9e",
            "1832a76f050a413ca20521a5190e9859",
            "bf54803dece84c9dae5534ca1b644a74",
            "bcf51503c1b24d90bb0a55741be97b09",
            "bf5fa255db2142be995a7d1341aa856d",
            "17d06225f5984c3982f2450c55364572",
            "6445ba572801423c9504d73d264bfd1a",
            "77d37dbd1e074d818197d94e01e135bb",
            "43f013046c884c93b5e34d0a5adb1938",
            "3fcb638bdf4f423da9e5971c8165c0ee",
            "511351586f974372b2f6c6c0e8df23c4",
            "6d247c1ba0794547bcd9fcfae1b618f8",
            "19582b9a881f41828594e4b6fc4b9596",
            "98e0e284046641deaa3ef4534f7e783a",
            "5cded7be708c49c389fb63d758a0a301",
            "b76db332f7224b0bb0f635772a962b7b",
            "c984b1ca6ba0458797be54aabe94ec13",
            "f423ccab28c04e64a7f97308f501323c",
            "6cd3993fa94b4d0992717522a5fdc337",
            "5e9b1f2267dc468fa58a58dd218b2259",
            "ec64916670654434adbc0c7aa18cc72c",
            "fb833fe923e94f49bd94c70c59bacdcc",
            "79220c70b8d04b9a9e17f5e84bc4868b",
            "29222ad878f44b078503f5c523133901",
            "d19102930790402f8ed902ab2f5db661"
          ]
        },
        "id": "J-oQOggK8C0N",
        "outputId": "40db6ede-7187-4cb7-d139-6fb404c1eb83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/09/2023 00:40:44 - INFO - __main__ - ***** Running training *****\n",
            "06/09/2023 00:40:44 - INFO - __main__ -   Num examples = 10000\n",
            "06/09/2023 00:40:44 - INFO - __main__ -   Num Epochs = 6\n",
            "06/09/2023 00:40:44 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "06/09/2023 00:40:44 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "06/09/2023 00:40:44 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "06/09/2023 00:40:44 - INFO - __main__ -   Total optimization steps = 60000\n",
            "06/09/2023 00:40:44 - INFO - accelerate.accelerator - Loading states from ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-5000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from checkpoint checkpoint-5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/09/2023 00:40:45 - INFO - accelerate.checkpointing - All model weights loaded successfully\n",
            "06/09/2023 00:40:45 - INFO - accelerate.checkpointing - All optimizer states loaded successfully\n",
            "06/09/2023 00:40:45 - INFO - accelerate.checkpointing - All scheduler states loaded successfully\n",
            "06/09/2023 00:40:45 - INFO - accelerate.checkpointing - GradScaler state loaded successfully\n",
            "06/09/2023 00:41:11 - INFO - accelerate.checkpointing - All random states loaded successfully\n",
            "06/09/2023 00:41:11 - INFO - accelerate.accelerator - Loading in 0 custom states\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "100ae2746dea4f6fb5f01578e8e11df3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/55000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "06/09/2023 01:10:43 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-10000\n",
            "06/09/2023 01:10:43 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-10000\\pytorch_model.bin\n",
            "06/09/2023 01:10:43 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-10000\\optimizer.bin\n",
            "06/09/2023 01:10:43 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-10000\\scheduler.bin\n",
            "06/09/2023 01:10:43 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-10000\\scaler.pt\n",
            "06/09/2023 01:10:43 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-10000\\random_states_0.pkl\n",
            "06/09/2023 01:10:43 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-10000\n",
            "06/09/2023 01:10:43 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "06/09/2023 01:11:38 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 01:38:10 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-15000\n",
            "06/09/2023 01:38:10 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-15000\\pytorch_model.bin\n",
            "06/09/2023 01:38:10 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-15000\\optimizer.bin\n",
            "06/09/2023 01:38:10 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-15000\\scheduler.bin\n",
            "06/09/2023 01:38:10 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-15000\\scaler.pt\n",
            "06/09/2023 01:38:10 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-15000\\random_states_0.pkl\n",
            "06/09/2023 01:38:10 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-15000\n",
            "06/09/2023 01:38:10 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 02:05:19 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-20000\n",
            "06/09/2023 02:05:19 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-20000\\pytorch_model.bin\n",
            "06/09/2023 02:05:19 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-20000\\optimizer.bin\n",
            "06/09/2023 02:05:19 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-20000\\scheduler.bin\n",
            "06/09/2023 02:05:19 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-20000\\scaler.pt\n",
            "06/09/2023 02:05:19 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-20000\\random_states_0.pkl\n",
            "06/09/2023 02:05:19 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-20000\n",
            "06/09/2023 02:05:19 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 02:06:11 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 02:32:37 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-25000\n",
            "06/09/2023 02:32:37 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-25000\\pytorch_model.bin\n",
            "06/09/2023 02:32:38 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-25000\\optimizer.bin\n",
            "06/09/2023 02:32:38 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-25000\\scheduler.bin\n",
            "06/09/2023 02:32:38 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-25000\\scaler.pt\n",
            "06/09/2023 02:32:38 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-25000\\random_states_0.pkl\n",
            "06/09/2023 02:32:38 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-25000\n",
            "06/09/2023 02:32:38 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 02:59:38 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-30000\n",
            "06/09/2023 02:59:38 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-30000\\pytorch_model.bin\n",
            "06/09/2023 02:59:38 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-30000\\optimizer.bin\n",
            "06/09/2023 02:59:38 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-30000\\scheduler.bin\n",
            "06/09/2023 02:59:39 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-30000\\scaler.pt\n",
            "06/09/2023 02:59:39 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-30000\\random_states_0.pkl\n",
            "06/09/2023 02:59:39 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-30000\n",
            "06/09/2023 02:59:39 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 03:00:30 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 03:26:55 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-35000\n",
            "06/09/2023 03:26:55 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-35000\\pytorch_model.bin\n",
            "06/09/2023 03:26:55 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-35000\\optimizer.bin\n",
            "06/09/2023 03:26:55 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-35000\\scheduler.bin\n",
            "06/09/2023 03:26:55 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-35000\\scaler.pt\n",
            "06/09/2023 03:26:56 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-35000\\random_states_0.pkl\n",
            "06/09/2023 03:26:56 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-35000\n",
            "06/09/2023 03:26:56 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 03:53:26 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-40000\n",
            "06/09/2023 03:53:26 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-40000\\pytorch_model.bin\n",
            "06/09/2023 03:53:26 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-40000\\optimizer.bin\n",
            "06/09/2023 03:53:26 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-40000\\scheduler.bin\n",
            "06/09/2023 03:53:26 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-40000\\scaler.pt\n",
            "06/09/2023 03:53:26 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-40000\\random_states_0.pkl\n",
            "06/09/2023 03:53:26 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-40000\n",
            "06/09/2023 03:53:26 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 03:54:17 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 04:20:05 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-45000\n",
            "06/09/2023 04:20:05 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-45000\\pytorch_model.bin\n",
            "06/09/2023 04:20:05 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-45000\\optimizer.bin\n",
            "06/09/2023 04:20:05 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-45000\\scheduler.bin\n",
            "06/09/2023 04:20:05 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-45000\\scaler.pt\n",
            "06/09/2023 04:20:05 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-45000\\random_states_0.pkl\n",
            "06/09/2023 04:20:05 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-45000\n",
            "06/09/2023 04:20:05 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 04:46:28 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-50000\n",
            "06/09/2023 04:46:28 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-50000\\pytorch_model.bin\n",
            "06/09/2023 04:46:28 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-50000\\optimizer.bin\n",
            "06/09/2023 04:46:28 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-50000\\scheduler.bin\n",
            "06/09/2023 04:46:28 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-50000\\scaler.pt\n",
            "06/09/2023 04:46:28 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-50000\\random_states_0.pkl\n",
            "06/09/2023 04:46:28 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-50000\n",
            "06/09/2023 04:46:28 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 04:47:18 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 05:14:10 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-55000\n",
            "06/09/2023 05:14:10 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-55000\\pytorch_model.bin\n",
            "06/09/2023 05:14:10 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-55000\\optimizer.bin\n",
            "06/09/2023 05:14:10 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-55000\\scheduler.bin\n",
            "06/09/2023 05:14:10 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-55000\\scaler.pt\n",
            "06/09/2023 05:14:10 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-55000\\random_states_0.pkl\n",
            "06/09/2023 05:14:10 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-55000\n",
            "06/09/2023 05:14:10 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 05:41:35 - INFO - accelerate.accelerator - Saving current state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-60000\n",
            "06/09/2023 05:41:35 - INFO - accelerate.checkpointing - Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-60000\\pytorch_model.bin\n",
            "06/09/2023 05:41:35 - INFO - accelerate.checkpointing - Optimizer state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-60000\\optimizer.bin\n",
            "06/09/2023 05:41:35 - INFO - accelerate.checkpointing - Scheduler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-60000\\scheduler.bin\n",
            "06/09/2023 05:41:35 - INFO - accelerate.checkpointing - Gradient scaler state saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-60000\\scaler.pt\n",
            "06/09/2023 05:41:35 - INFO - accelerate.checkpointing - Random states saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-60000\\random_states_0.pkl\n",
            "06/09/2023 05:41:35 - INFO - __main__ - Saved state to ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\checkpoint-60000\n",
            "06/09/2023 05:41:35 - INFO - __main__ - Running sampling... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n",
            "06/09/2023 05:42:25 - INFO - __main__ - Running validation... \n",
            " Generating 3 images with prompt: a high-quality, detailed, and professional image.\n",
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n"
          ]
        }
      ],
      "source": [
        "# Train!\n",
        "total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "global_step = 0\n",
        "first_epoch = 0\n",
        "\n",
        "# Potentially load in the weights and states from a previous save\n",
        "if args.resume_from_checkpoint:\n",
        "    if args.resume_from_checkpoint != \"latest\":\n",
        "        path = os.path.basename(args.resume_from_checkpoint)\n",
        "    else:\n",
        "        # Get the most recent checkpoint\n",
        "        dirs = os.listdir(args.output_dir)\n",
        "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
        "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "        path = dirs[-1] if len(dirs) > 0 else None\n",
        "\n",
        "    if path is None:\n",
        "        accelerator.print(\n",
        "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
        "        )\n",
        "        args.resume_from_checkpoint = None\n",
        "    else:\n",
        "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
        "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
        "        global_step = int(path.split(\"-\")[1])\n",
        "\n",
        "        resume_global_step = global_step * args.gradient_accumulation_steps\n",
        "        first_epoch = global_step // num_update_steps_per_epoch\n",
        "        resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\n",
        "\n",
        "# Only show the progress bar once on each machine.\n",
        "progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "progress_bar.set_description(\"Steps\")\n",
        "\n",
        "for epoch in range(first_epoch, args.num_train_epochs):\n",
        "    unet.train()\n",
        "    train_loss = 0.0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Skip steps until we reach the resumed step\n",
        "        if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
        "            if step % args.gradient_accumulation_steps == 0:\n",
        "                progress_bar.update(1)\n",
        "            continue\n",
        "\n",
        "        with accelerator.accumulate(unet):\n",
        "            # Convert images to latent space\n",
        "            latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
        "            latents = latents * vae.config.scaling_factor\n",
        "\n",
        "            # Sample noise that we'll add to the latents\n",
        "            noise = torch.randn_like(latents)\n",
        "            bsz = latents.shape[0]\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)\n",
        "            timesteps = timesteps.long()\n",
        "\n",
        "            # Add noise to the latents according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Get the text embedding for conditioning\n",
        "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "            # Inject control states to unet\n",
        "            _ = control_lora(batch[\"guide_values\"].to(dtype=weight_dtype)).control_states\n",
        "\n",
        "            # Get the target for loss depending on the prediction type\n",
        "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                target = noise\n",
        "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "            # Predict the noise residual and compute loss\n",
        "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "            # Gather the losses across all processes for logging (if we use distributed training).\n",
        "            avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
        "            train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
        "\n",
        "            # Backpropagate\n",
        "            accelerator.backward(loss)\n",
        "            if accelerator.sync_gradients:\n",
        "                params_to_clip = control_lora.parameters()\n",
        "                accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "        if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
        "            train_loss = 0.0\n",
        "\n",
        "            if global_step % args.checkpointing_steps == 0:\n",
        "                if accelerator.is_main_process:\n",
        "                    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
        "                    accelerator.save_state(save_path)\n",
        "                    logger.info(f\"Saved state to {save_path}\")\n",
        "\n",
        "                    if args.validation_prompt is not None:\n",
        "                        logger.info(\n",
        "                            f\"Running sampling... \\n Generating {args.num_validation_images} images with prompt:\"\n",
        "                            f\" {args.validation_prompt}.\"\n",
        "                        )\n",
        "                        # create pipeline\n",
        "                        pipeline = DiffusionPipeline.from_pretrained(\n",
        "                            args.pretrained_model_name_or_path,\n",
        "                            unet=accelerator.unwrap_model(unet),\n",
        "                            revision=args.revision,\n",
        "                            torch_dtype=weight_dtype,\n",
        "                            safety_checker=None\n",
        "                        )\n",
        "                        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "                        pipeline = pipeline.to(accelerator.device)\n",
        "                        pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "                        # run inference\n",
        "                        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
        "                        images = []\n",
        "                        for _ in range(args.num_validation_images):\n",
        "                            with torch.no_grad():\n",
        "                                try:\n",
        "                                    batch = next(val_iter)\n",
        "                                except:\n",
        "                                    val_iter = iter(val_dataloader)\n",
        "                                    batch = next(val_iter)\n",
        "                                target = batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
        "                                guide = batch[\"guide_values\"].to(accelerator.device)\n",
        "                                _ = control_lora(guide).control_states\n",
        "                                image = pipeline(\n",
        "                                    args.validation_prompt, num_inference_steps=30, generator=generator).images[0]\n",
        "                                image = dataset_cls.cat_input(image, target, guide)\n",
        "                            images.append(image)\n",
        "\n",
        "                        for tracker in accelerator.trackers:\n",
        "                            if tracker.name == \"tensorboard\":\n",
        "                                np_images = np.stack([np.asarray(img) for img in images])\n",
        "                                tracker.writer.add_images(\"sampling\", np_images, epoch, dataformats=\"NHWC\")\n",
        "                            if tracker.name == \"wandb\":\n",
        "                                tracker.log(\n",
        "                                    {\n",
        "                                        \"sampling\": [\n",
        "                                            wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\n",
        "                                            for i, image in enumerate(images)\n",
        "                                        ]\n",
        "                                    }\n",
        "                                )\n",
        "\n",
        "                        del pipeline\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "        logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "        progress_bar.set_postfix(**logs)\n",
        "\n",
        "        if global_step >= args.max_train_steps:\n",
        "            break\n",
        "    \n",
        "    if accelerator.is_main_process:\n",
        "        if args.validation_prompt is not None and epoch % args.validation_epochs == 0:\n",
        "            logger.info(\n",
        "                f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
        "                f\" {args.validation_prompt}.\"\n",
        "            )\n",
        "            # create pipeline\n",
        "            pipeline = DiffusionPipeline.from_pretrained(\n",
        "                args.pretrained_model_name_or_path,\n",
        "                unet=accelerator.unwrap_model(unet),\n",
        "                revision=args.revision,\n",
        "                torch_dtype=weight_dtype,\n",
        "                safety_checker=None\n",
        "            )\n",
        "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "            pipeline = pipeline.to(accelerator.device)\n",
        "            pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "            # run inference\n",
        "            generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
        "            images = []\n",
        "            for _ in range(args.num_validation_images):\n",
        "                with torch.no_grad():\n",
        "                    try:\n",
        "                        batch = next(val_iter)\n",
        "                    except:\n",
        "                        val_iter = iter(val_dataloader)\n",
        "                        batch = next(val_iter)\n",
        "                    target = batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
        "                    guide = batch[\"guide_values\"].to(accelerator.device)\n",
        "                    _ = control_lora(guide).control_states\n",
        "                    image = pipeline(args.validation_prompt, num_inference_steps=30, generator=generator).images[0]\n",
        "                    image = dataset_cls.cat_input(image, target, guide)\n",
        "                images.append(image)\n",
        "\n",
        "            if accelerator.is_main_process:\n",
        "                for tracker in accelerator.trackers:\n",
        "                    if tracker.name == \"tensorboard\":\n",
        "                        np_images = np.stack([np.asarray(img) for img in images])\n",
        "                        tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\n",
        "                    if tracker.name == \"wandb\":\n",
        "                        tracker.log(\n",
        "                            {\n",
        "                                \"validation\": [\n",
        "                                    wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\n",
        "                                    for i, image in enumerate(images)\n",
        "                                ]\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "            del pipeline\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving the model to huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87c6de698f434bccb109c4694a58432a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\BUE\\Graduation Project\\MyWork\\ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora is already a clone of https://huggingface.co/wtcherr/sd-unsplash_10k_blur_rand_KS-model-control-lora. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "06/09/2023 07:09:21 - WARNING - huggingface_hub.repository - d:\\BUE\\Graduation Project\\MyWork\\ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora is already a clone of https://huggingface.co/wtcherr/sd-unsplash_10k_blur_rand_KS-model-control-lora. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "Configuration saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\config.json\n",
            "Configuration saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\config.json\n",
            "Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\diffusion_pytorch_model.bin\n",
            "Configuration saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\config.json\n",
            "Model weights saved in ckpts/sd-unsplash_10k_blur_rand_KS-model-control-lora\\diffusion_pytorch_model.safetensors\n",
            "Adding files tracked by Git LFS: ['image_0.png', 'image_1.png', 'image_2.png']. This may take a bit of time if the files are large.\n",
            "06/09/2023 07:09:28 - WARNING - huggingface_hub.repository - Adding files tracked by Git LFS: ['image_0.png', 'image_1.png', 'image_2.png']. This may take a bit of time if the files are large.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0992c46140be4ab98cafeff647e6ed99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-40000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac53d1b24f324b7d89cad2137ecb6af3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-30000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e656d4a0d4a543968b00a746debf1dac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-10000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84b656f041474e118bce726d3d46c8a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-35000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0b6f6eab2ef413ca62d68bf35cf7850",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-5000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13161787090b4a86a685bd541e7a6dab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-45000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b740a982d7e47059714b4e8d0d55ac2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-15000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41779a1ceeca459eb997de3f8b329cc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-55000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "580a87c1141c42e7bc845142a061ccd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-50000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58b3a5407b024c99a084d1b89d21db55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-20000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cea8c0eb592149b993c0bae4e86d3e36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-25000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "212d8909be364d7d8bcccb98892c8f74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-60000/optimizer.bin:   0%|          | 1.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6977717c1a454dc99af787843cf448d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file diffusion_pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b31a53dc68514c8e810ed0c1f5a6f6d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-30000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16a70cfe92b04b7ca415f18053e88b8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-45000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85dd297b8a1943199592e5c7896b8e99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-20000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cad54bc1c74e4c55957c66e433359b83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-25000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb10a645538843d7a0f04ce81a129255",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-50000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5685830d90b42469700afdab03e50d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-60000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a14665c1ec0e4bc18f15b999fe00d6ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-5000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d695626e56c4036b6a480f346c77516",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-40000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2de490f8149942d19ddac9a7b05818eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-15000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b26662d759f4e80a10c5859a7d0c6f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-10000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d50dc7ac83a146c08926385bebf0816a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-55000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15fb2705c09849f48aadfd197341b967",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-35000/pytorch_model.bin:   0%|          | 1.00/23.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "899454055c7b495384db3ed817d4ed52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file diffusion_pytorch_model.safetensors:   0%|          | 1.00/23.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "019362e1b11c4429bad8d5ad191ea78e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file image_1.png:   0%|          | 1.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cbd38530bb449b5812bc47330aee0ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file image_0.png:   0%|          | 1.00/915k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6640910747f24995ae9046275e7a34d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file image_2.png:   0%|          | 1.00/822k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "083c62adb74f4727bedac0eea44e0d67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-40000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b86789917144dbdb8166141118d8166",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-5000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43b4c290ab540e4b18f6353f1a80b8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-10000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffab193927c14dbc81782f6789ce44e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-35000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fafce631ec54321aecb5365cd25b049",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-15000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d954ac25d92948689d3ad6775deef315",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-60000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "706d189ce78b4cd9b2a48e096415fd09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-20000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "976b23e857d24bf984ea3d21580b95dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-55000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f00581a20bfe4d62ac1815260e1c292b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-45000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3c72e4439c646b5bf659596a20e58a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-50000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a8708b00769464eaef2cd1cda1b31f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-25000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cea6e592a8e41fd90588bd79789cfcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-30000/random_states_0.pkl:   0%|          | 1.00/14.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec7fc36aa7ea4eab8077bbfa6c0d6709",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-55000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "477960d636dc4726bf660bc244474439",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-35000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9b91308ed3a4c40a15cb901fda7901d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-10000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ca01384c768481c95f039ce245643ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-5000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "702099bf23d846c68bf465adf5b132a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-60000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67a297023d724457a19908653f65d27b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-45000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "439ec2369ff441cb8342e8ee85aa7cb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-25000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5938073910fb467d88781b5a136456bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-40000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e32b5f9c71a848008c480cf85e9e7229",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-50000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "630cc05aceca40248abcb3c659e875f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-20000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "510d435b7b734e1493f90193b879cf5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-15000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "affdef454b7e4ffdae0966865c7d8cd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-30000/scheduler.bin:   0%|          | 1.00/563 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "161b41ce6c5148528e45824c3e766af0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-45000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b089828898a94ed1b72d6e45cbf04e9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-55000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf5e43835ca0449ca9f518515988f540",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-30000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "021d568a340e4d4497c88e7daf67c0c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-20000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cd84dfd7e124bd08988836cf280f2d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-50000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daa9105e9b484ecdbdd7fa3cf6e0dec2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-60000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "653ea5b2599a42f38244d65b1e5dca57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-25000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd846ad2756f46138ba32fdffb109a6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-15000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3e4724d9c2143cc8f539ea4cfca253d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-40000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cb6138f062445d1a8a0333d36d315ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-5000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2bdad1caf73455a8d8fddbf0859b420",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-10000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3df2e8aedba84e5b8a5ef57079e59d95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file checkpoint-35000/scaler.pt:   0%|          | 1.00/557 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "To https://huggingface.co/wtcherr/sd-unsplash_10k_blur_rand_KS-model-control-lora\n",
            "   3e18b08..23b198d  main -> main\n",
            "\n",
            "06/09/2023 07:15:57 - WARNING - huggingface_hub.repository - To https://huggingface.co/wtcherr/sd-unsplash_10k_blur_rand_KS-model-control-lora\n",
            "   3e18b08..23b198d  main -> main\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Save the lora layers\n",
        "accelerator.wait_for_everyone()\n",
        "if accelerator.is_main_process:\n",
        "    repo = Repository(args.output_dir, clone_from=repo_name)\n",
        "    unet = unet.to(torch.float32)\n",
        "    # unet.save_attn_procs(args.output_dir)\n",
        "    control_lora.save_config(args.output_dir)\n",
        "    control_lora.save_pretrained(args.output_dir, safe_serialization=False)\n",
        "    control_lora.save_pretrained(args.output_dir, safe_serialization=True)\n",
        "\n",
        "    if args.push_to_hub:\n",
        "        save_model_card(\n",
        "            repo_name,\n",
        "            images=images,\n",
        "            base_model=args.pretrained_model_name_or_path,\n",
        "            dataset_name=args.dataset_name,\n",
        "            repo_folder=args.output_dir,\n",
        "        )\n",
        "        repo.git_add(auto_lfs_track=True)\n",
        "        repo.git_commit(commit_message='End of training')\n",
        "        repo.git_push()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M94uiNBB8C0O"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "{'projection_class_embeddings_input_dim', 'class_embeddings_concat', 'conv_in_kernel', 'cross_attention_norm', 'encoder_hid_dim', 'conv_out_kernel', 'class_embed_type', 'resnet_time_scale_shift', 'time_embedding_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_cond_proj_dim', 'mid_block_type', 'addition_embed_type_num_heads', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'num_class_embeds', 'time_embedding_type', 'addition_embed_type', 'resnet_skip_time_act', 'use_linear_projection', 'only_cross_attention', 'upcast_attention', 'timestep_post_act'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'sample_max_value', 'thresholding', 'solver_type', 'dynamic_thresholding_ratio', 'algorithm_type', 'variance_type', 'lower_order_final', 'use_karras_sigmas', 'solver_order'} was not found in config. Values will be initialized to default values.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8e467ea6213489cae4bdd717cf0ab99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92bf9fdca6f04fb3a0761205148c2dae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7623c34ef0b495e81bde7054d88e846",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▂▁▁▃█▂█▃▁▁▁▁▁▃▁▁▂▂▂▁▅▁▄▂▄▃▁▃▁▅▁▂▃▁▁▁▄▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.15092</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vocal-moon-2</strong> at: <a href='https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS/runs/maqyhey0' target=\"_blank\">https://wandb.ai/wtcherrr/ControlLora_unsplash_10k_blur_rand_KS/runs/maqyhey0</a><br/>Synced 6 W&B file(s), 54 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230609_004035-maqyhey0\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Final inference\n",
        "# Load previous pipeline\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    args.pretrained_model_name_or_path, revision=args.revision, torch_dtype=weight_dtype, safety_checker=None\n",
        ")\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "pipeline = pipeline.to(accelerator.device)\n",
        "\n",
        "# load attention processors\n",
        "lora_attn_procs = {}\n",
        "lora_layers_list = list([list(layer_list) for layer_list in control_lora.lora_layers])\n",
        "for name in pipeline.unet.attn_processors.keys():\n",
        "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "    if name.startswith(\"mid_block\"):\n",
        "        control_id = control_ids[-1]\n",
        "    elif name.startswith(\"up_blocks\"):\n",
        "        block_id = int(name[len(\"up_blocks.\")])\n",
        "        control_id = list(reversed(control_ids))[block_id]\n",
        "    elif name.startswith(\"down_blocks\"):\n",
        "        block_id = int(name[len(\"down_blocks.\")])\n",
        "        control_id = control_ids[block_id]\n",
        "\n",
        "    lora_layers = lora_layers_list[control_id]\n",
        "    if len(lora_layers) != 0:\n",
        "        lora_layer = lora_layers.pop(0)\n",
        "        lora_attn_procs[name] = lora_layer\n",
        "\n",
        "pipeline.unet.set_attn_processor(lora_attn_procs)\n",
        "\n",
        "# run inference\n",
        "generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
        "images = []\n",
        "for _ in range(args.num_validation_images):\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            batch = next(val_iter)\n",
        "        except:\n",
        "            val_iter = iter(val_dataloader)\n",
        "            batch = next(val_iter)\n",
        "        target = batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
        "        guide = batch[\"guide_values\"].to(accelerator.device)\n",
        "        _ = control_lora(guide).control_states\n",
        "        image = pipeline(args.validation_prompt, num_inference_steps=30, generator=generator).images[0]\n",
        "        image = dataset_cls.cat_input(image, target, guide)\n",
        "    images.append(image)\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    for tracker in accelerator.trackers:\n",
        "        if tracker.name == \"tensorboard\":\n",
        "            np_images = np.stack([np.asarray(img) for img in images])\n",
        "            tracker.writer.add_images(\"test\", np_images, epoch, dataformats=\"NHWC\")\n",
        "        if tracker.name == \"wandb\":\n",
        "            tracker.log(\n",
        "                {\n",
        "                    \"test\": [\n",
        "                        wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\n",
        "                        for i, image in enumerate(images)\n",
        "                    ]\n",
        "                }\n",
        "            )\n",
        "\n",
        "accelerator.end_training()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rt4RCGn98C0O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "def HWC3(x):\n",
        "    assert x.dtype == np.uint8\n",
        "    if x.ndim == 2:\n",
        "        x = x[:, :, None]\n",
        "    assert x.ndim == 3\n",
        "    H, W, C = x.shape\n",
        "    assert C == 1 or C == 3 or C == 4\n",
        "    if C == 3:\n",
        "        return x\n",
        "    if C == 1:\n",
        "        return np.concatenate([x, x, x], axis=2)\n",
        "    if C == 4:\n",
        "        color = x[:, :, 0:3].astype(np.float32)\n",
        "        alpha = x[:, :, 3:4].astype(np.float32) / 255.0\n",
        "        y = color * alpha + 255.0 * (1.0 - alpha)\n",
        "        y = y.clip(0, 255).astype(np.uint8)\n",
        "        return y\n",
        "\n",
        "\n",
        "def resize_image(input_image, resolution):\n",
        "    H, W, C = input_image.shape\n",
        "    H = float(H)\n",
        "    W = float(W)\n",
        "    k = float(resolution) / min(H, W)\n",
        "    H *= k\n",
        "    W *= k\n",
        "    H = int(np.round(H / 64.0)) * 64\n",
        "    W = int(np.round(W / 64.0)) * 64\n",
        "    img = cv2.resize(input_image, (W, H), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n",
        "    return img\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Canny Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "EXdZLuBN8C0O"
      },
      "outputs": [],
      "source": [
        "class CannyDetector:\n",
        "    def __call__(self, img, low_threshold, high_threshold):\n",
        "        return cv2.Canny(img, low_threshold, high_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Hhjxpq528C0O",
        "outputId": "0fe79b84-b151-48fa-d557-86943ad1c18d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vae\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "{'timestep_post_act', 'cross_attention_norm', 'mid_block_type', 'num_class_embeds', 'mid_block_only_cross_attention', 'use_linear_projection', 'time_embedding_act_fn', 'addition_embed_type', 'resnet_out_scale_factor', 'addition_embed_type_num_heads', 'time_embedding_dim', 'dual_cross_attention', 'conv_in_kernel', 'time_embedding_type', 'projection_class_embeddings_input_dim', 'class_embeddings_concat', 'resnet_time_scale_shift', 'only_cross_attention', 'time_cond_proj_dim', 'encoder_hid_dim', 'conv_out_kernel', 'class_embed_type', 'upcast_attention', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'lambda_min_clipped', 'dynamic_thresholding_ratio', 'algorithm_type', 'lower_order_final', 'variance_type', 'thresholding', 'solver_type', 'use_karras_sigmas', 'solver_order', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\gradio\\components.py:4417: UserWarning: The 'grid' parameter will be deprecated. Please use 'columns' instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://0.0.0.0:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daf7ce2e04f34936b45d2cdc6926e1fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fb88640e39d4bc5bbc73ea4a156e261",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad462ed72cda4c8c94c4bad495d74c04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35feb0f73d7649468a442cab9b5d9859",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ff55a434ee5440ca4841052c0d56b14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c574c38227f4e279d6159f5a44e05e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf3c028819af46d08a911fc1baf7e02a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2afc86612c654828a806200489d9a2c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf8a395d12be45d5b087cac162ebf489",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a43bd1d765f47869f9d309c46b62d19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84cea5479da044418cad1ad4644ecac1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaf8d43a98504dad966fd8813e794779",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bdd084faa5c4ec99d7e3ff6acd23a2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c4e4a35c8374de089e4e47e56150912",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5d0157e0d3644e6836f6d31a3d3ca0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d439ed91c25c4866b7e1aaa562735bf8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63c0660782e941afa6412bc59da4dd61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52cc06a1f85149048e982dfaee0436b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97dcc5f8569142f2880c9f502e7d42a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f763f1ddb9c1407dbd460653ae447da6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b855c57669f4f3ca47f9026fc086e91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b8056ceed6a484bafaa9d4e431f5e7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa95c0abca864c79a8855f7903c64268",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a17e7eb975cc45ceaed9b8fd2c4b6ac3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d372c1aae02043c9badcf8ca57598961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "\n",
        "from diffusers import utils\n",
        "from diffusers.utils import deprecation_utils\n",
        "from diffusers.models import cross_attention\n",
        "utils.deprecate = lambda *arg, **kwargs: None\n",
        "deprecation_utils.deprecate = lambda *arg, **kwargs: None\n",
        "cross_attention.deprecate = lambda *arg, **kwargs: None\n",
        "\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
        "from diffusers.pipelines import DiffusionPipeline\n",
        "from diffusers.schedulers import DPMSolverMultistepScheduler\n",
        "\n",
        "apply_canny = CannyDetector()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    'runwayml/stable-diffusion-v1-5', safety_checker=None\n",
        ")\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "pipeline = pipeline.to(device)\n",
        "unet: UNet2DConditionModel = pipeline.unet\n",
        "\n",
        "control_lora = ControlLoRA.from_pretrained('ckpts/sd-unsplash_5k_canny-model-control-lora')\n",
        "control_lora = control_lora.to(device)\n",
        "\n",
        "\n",
        "# load control lora attention processors\n",
        "lora_attn_procs = {}\n",
        "lora_layers_list = list([list(layer_list) for layer_list in control_lora.lora_layers])\n",
        "n_ch = len(unet.config.block_out_channels)\n",
        "control_ids = [i for i in range(n_ch)]\n",
        "for name in pipeline.unet.attn_processors.keys():\n",
        "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "    if name.startswith(\"mid_block\"):\n",
        "        control_id = control_ids[-1]\n",
        "    elif name.startswith(\"up_blocks\"):\n",
        "        block_id = int(name[len(\"up_blocks.\")])\n",
        "        control_id = list(reversed(control_ids))[block_id]\n",
        "    elif name.startswith(\"down_blocks\"):\n",
        "        block_id = int(name[len(\"down_blocks.\")])\n",
        "        control_id = control_ids[block_id]\n",
        "\n",
        "    lora_layers = lora_layers_list[control_id]\n",
        "    if len(lora_layers) != 0:\n",
        "        lora_layer: ControlLoRACrossAttnProcessor = lora_layers.pop(0)\n",
        "        lora_attn_procs[name] = lora_layer\n",
        "\n",
        "unet.set_attn_processor(lora_attn_procs)\n",
        "\n",
        "def save(*args):\n",
        "    # unpack arguments\n",
        "    input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, low_threshold, high_threshold, result_images = args\n",
        "\n",
        "    # check if result_images is empty\n",
        "    if not result_images:\n",
        "        return\n",
        "\n",
        "    # create save directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    save_folder = os.path.join(model_name, \"saves\", timestamp)\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # Saving parameters\n",
        "    with open(os.path.join(save_folder, \"parameters.txt\"), \"w\") as f:\n",
        "        params = [prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, low_threshold, high_threshold]\n",
        "        param_names = [\"prompt\", \"a_prompt\", \"n_prompt\", \"num_samples\", \"image_resolution\", \"sample_steps\", \"scale\", \"seed\", \"eta\", \"low_threshold\", \"high_threshold\"]\n",
        "        for name, value in zip(param_names, params):\n",
        "            f.write(f\"{name}: {value}\\n\")\n",
        "\n",
        "    # save original image\n",
        "    input_image_pil = Image.fromarray(input_image)\n",
        "    input_image_pil.save(os.path.join(save_folder, \"original_image.png\"))\n",
        "\n",
        "    # Saving images\n",
        "    for i, result_image_dict in enumerate(result_images):\n",
        "        # get the filename of the result_image\n",
        "        result_image_filename = result_image_dict['name']\n",
        "        # read the image file as a numpy array\n",
        "        result_image_np = cv2.imread(result_image_filename)\n",
        "        # convert the numpy array to a PIL image\n",
        "        result_image_pil = Image.fromarray(cv2.cvtColor(result_image_np, cv2.COLOR_BGR2RGB))\n",
        "        if i == 0:\n",
        "            # save the guide image\n",
        "            result_image_pil.save(os.path.join(save_folder, \"guide_image.png\"))\n",
        "        else:\n",
        "            # save the generated image\n",
        "            result_image_pil.save(os.path.join(save_folder, f\"generated_image_{i}.png\"))\n",
        "\n",
        "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, low_threshold, high_threshold):\n",
        "    with torch.no_grad():\n",
        "        img = resize_image(HWC3(input_image), image_resolution)\n",
        "        H, W, C = img.shape\n",
        "\n",
        "        detected_map = apply_canny(img, low_threshold, high_threshold)\n",
        "        detected_map = HWC3(detected_map)\n",
        "\n",
        "        control = torch.from_numpy(detected_map[...,::-1].copy().transpose([2,0,1])).float().to(device)[None] / 127.5 - 1\n",
        "        _ = control_lora(control).control_states\n",
        "\n",
        "        if seed == -1:\n",
        "            seed = random.randint(0, 65535)\n",
        "\n",
        "        # run inference\n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        images = []\n",
        "        for i in range(num_samples):\n",
        "            _ = control_lora(control).control_states\n",
        "            image = pipeline(\n",
        "                prompt + ', ' + a_prompt, negative_prompt=n_prompt, \n",
        "                num_inference_steps=sample_steps, guidance_scale=scale, eta=eta,\n",
        "                generator=generator, height=H, width=W).images[0]\n",
        "            images.append(np.asarray(image))\n",
        "        \n",
        "        results = images\n",
        "    return [255 - detected_map] + results\n",
        "\n",
        "\n",
        "block = gr.Blocks().queue()\n",
        "with block:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"## Control Stable Diffusion with Canny Edge Maps\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_image = gr.Image(source='upload', type=\"numpy\")\n",
        "            prompt = gr.Textbox(label=\"Prompt\")\n",
        "            run_button = gr.Button(label=\"Run\")\n",
        "            save_button = gr.Button(label=\"Save\")\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "                num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
        "                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=256)\n",
        "                low_threshold = gr.Slider(label=\"Canny low threshold\", minimum=1, maximum=10, value=5, step=1)\n",
        "                high_threshold = gr.Slider(label=\"Canny high threshold\", minimum=130, maximum=150, value=140, step=1)\n",
        "                sample_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n",
        "                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n",
        "                seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n",
        "                eta = gr.Number(label=\"eta\", value=0.0)\n",
        "                a_prompt = gr.Textbox(label=\"Added Prompt\", value='a high-quality, detailed, and professional image')\n",
        "                n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
        "                                      value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
        "        with gr.Column():\n",
        "            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n",
        "    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, low_threshold, high_threshold]\n",
        "    ips_save = ips + [result_gallery]\n",
        "    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n",
        "    save_button.click(fn=save, inputs=ips_save, outputs=[])\n",
        "\n",
        "\n",
        "block.launch(server_name='0.0.0.0', server_port=7860)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "block.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Blur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GaussianBlur:\n",
        "    def __call__(self, img, kernel_size, sigmaX):\n",
        "        return cv2.GaussianBlur(img, (kernel_size, kernel_size), sigmaX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unet\\diffusion_pytorch_model.safetensors not found\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "{'addition_embed_type', 'num_class_embeds', 'cross_attention_norm', 'use_linear_projection', 'conv_out_kernel', 'class_embeddings_concat', 'time_embedding_dim', 'only_cross_attention', 'upcast_attention', 'resnet_time_scale_shift', 'time_embedding_act_fn', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'dual_cross_attention', 'addition_embed_type_num_heads', 'resnet_skip_time_act', 'timestep_post_act', 'time_embedding_type', 'mid_block_type', 'conv_in_kernel', 'class_embed_type', 'encoder_hid_dim', 'projection_class_embeddings_input_dim', 'time_cond_proj_dim'} was not found in config. Values will be initialized to default values.\n",
            "{'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "{'solver_type', 'thresholding', 'algorithm_type', 'dynamic_thresholding_ratio', 'solver_order', 'sample_max_value', 'lambda_min_clipped', 'lower_order_final', 'use_karras_sigmas', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\gradio\\components.py:4417: UserWarning: The 'grid' parameter will be deprecated. Please use 'columns' instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://0.0.0.0:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0ed88e26f934ba0ba6e666df34bc864",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "926d3ab2eb18430fb7618f315c1042a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b40438ec141f4eaaa6da443a426ab1cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e22720ce9a4042d48a2ce62255999335",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23f8c0dab5d14f209499fe1c539addc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56f76d545b974696b1ab71e3c4b1af8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3771199f641e4f2a94d4897d087ef1b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0ce32b1298443e58c24e556044778b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baaa915aaeb84ecb9e14489c87928556",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "350c8f5f015d4e968b263134286be049",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2132beadedc6416e80aa734ef060f85f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf60e0339902497e8b7fc22452c4662f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "828ab0606e6147b7840203ff2643eee8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eaf12f3a1f542c096e69bf0bb8c033e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0eec0e4f05ff49a5b3503b51bad1f269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d48d63b11dfa4ede8a4f2cf55b25a98e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd6c5b3b3e9948e7983b2cf315607cc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a2ca9c3caa476fb831deaf4d602542",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c99b86a178f446ff9c4024f15e867816",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7897120cb67e414a9fecdb1786f49ece",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca47499b45684e25adcb5fb86d4497a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfaa2a41fce44db68ecc19d5c3dad10e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94aebff500254f91a79bebb67cd48337",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "642c1353d23041f8b8f1917831bce5e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3c7e253fb5e4c908a1958b4d4798e16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b11a4b1d05c4de5b3cfd4b33c0c3167",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1928e2eef23644adaa399ab4ce23eb31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db810a0e539f451f9a7371491db47e00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11a3e210db424af3acb52dc0e9c4c316",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f7af84d1e3c479a993afa1229a814c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c795f3cd21d94f7aa61cab12afdc3b13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20e50002ed414ca7bd57e5f988c803ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa6d5c4694544a879657aed3855c35b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "995bf7b726084c7a988bf704c66334cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c997e48f40694e96afe835aa4758f489",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09d75776c63247ac9aa436ef3476ab7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3491a77a6ae0404fa848fa00defe1c9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a752666059f42dc925f1c43dcc41b02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48157c3bb9aa4e498c136c54538da943",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80a2b6bb9b344aa8b0deeb1100c6272e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67aefaa7461c45de9663aa99ddf6ff10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\configuration_utils.py:135: FutureWarning: Accessing config attribute `lambda_min_clipped` directly via 'DPMSolverMultistepScheduler' object attribute is deprecated. Please access 'lambda_min_clipped' over 'DPMSolverMultistepScheduler's config object instead, e.g. 'scheduler.config.lambda_min_clipped'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4e4e2178aec4d97ac1b6d01407b7c67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d92f526dc244436a4865d517afbe0b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8ba1e321d8d4f01a63c6177b31302b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c1a1d8feffa49d9842599cc3c749505",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59aacf27530948bc9fd963dc22e9d535",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "\n",
        "from diffusers import utils\n",
        "from diffusers.utils import deprecation_utils\n",
        "from diffusers.models import cross_attention\n",
        "utils.deprecate = lambda *arg, **kwargs: None\n",
        "deprecation_utils.deprecate = lambda *arg, **kwargs: None\n",
        "cross_attention.deprecate = lambda *arg, **kwargs: None\n",
        "\n",
        "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
        "from diffusers.pipelines import DiffusionPipeline\n",
        "from diffusers.schedulers import DPMSolverMultistepScheduler\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "apply_gaussian_blur = GaussianBlur()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    'runwayml/stable-diffusion-v1-5', safety_checker=None\n",
        ")\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "pipeline = pipeline.to(device)\n",
        "unet: UNet2DConditionModel = pipeline.unet\n",
        "\n",
        "control_lora = ControlLoRA.from_pretrained('ckpts\\sd-unsplash_5k_blur_61KS-model-control-lora')\n",
        "control_lora = control_lora.to(device)\n",
        "\n",
        "\n",
        "# load control lora attention processors\n",
        "lora_attn_procs = {}\n",
        "lora_layers_list = list([list(layer_list) for layer_list in control_lora.lora_layers])\n",
        "n_ch = len(unet.config.block_out_channels)\n",
        "control_ids = [i for i in range(n_ch)]\n",
        "for name in pipeline.unet.attn_processors.keys():\n",
        "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "    if name.startswith(\"mid_block\"):\n",
        "        control_id = control_ids[-1]\n",
        "    elif name.startswith(\"up_blocks\"):\n",
        "        block_id = int(name[len(\"up_blocks.\")])\n",
        "        control_id = list(reversed(control_ids))[block_id]\n",
        "    elif name.startswith(\"down_blocks\"):\n",
        "        block_id = int(name[len(\"down_blocks.\")])\n",
        "        control_id = control_ids[block_id]\n",
        "\n",
        "    lora_layers = lora_layers_list[control_id]\n",
        "    if len(lora_layers) != 0:\n",
        "        lora_layer: ControlLoRACrossAttnProcessor = lora_layers.pop(0)\n",
        "        lora_attn_procs[name] = lora_layer\n",
        "\n",
        "unet.set_attn_processor(lora_attn_procs)\n",
        "\n",
        "\n",
        "def save(*args):\n",
        "    # unpack arguments\n",
        "    input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX, result_images = args\n",
        "\n",
        "    # check if result_images is empty\n",
        "    if not result_images:\n",
        "        return\n",
        "\n",
        "    # create save directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    save_folder = os.path.join(model_name, \"saves\", timestamp)\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # Saving parameters\n",
        "    with open(os.path.join(save_folder, \"parameters.txt\"), \"w\") as f:\n",
        "        params = [prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX]\n",
        "        param_names = [\"prompt\", \"a_prompt\", \"n_prompt\", \"num_samples\", \"image_resolution\", \"sample_steps\", \"scale\", \"seed\", \"eta\", \"kernel_size\", \"sigmaX\"]\n",
        "        for name, value in zip(param_names, params):\n",
        "            f.write(f\"{name}: {value}\\n\")\n",
        "\n",
        "    # save original image\n",
        "    input_image_pil = Image.fromarray(input_image)\n",
        "    input_image_pil.save(os.path.join(save_folder, \"original_image.png\"))\n",
        "\n",
        "    # Saving images\n",
        "    for i, result_image_dict in enumerate(result_images):\n",
        "        # get the filename of the result_image\n",
        "        result_image_filename = result_image_dict['name']\n",
        "        # read the image file as a numpy array\n",
        "        result_image_np = cv2.imread(result_image_filename)\n",
        "        # convert the numpy array to a PIL image\n",
        "        result_image_pil = Image.fromarray(cv2.cvtColor(result_image_np, cv2.COLOR_BGR2RGB))\n",
        "        if i == 0:\n",
        "            # save the guide image\n",
        "            result_image_pil.save(os.path.join(save_folder, \"guide_image.png\"))\n",
        "        else:\n",
        "            # save the generated image\n",
        "            result_image_pil.save(os.path.join(save_folder, f\"generated_image_{i}.png\"))\n",
        "\n",
        "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX):\n",
        "    with torch.no_grad():\n",
        "        img = resize_image(HWC3(input_image), image_resolution)\n",
        "        H, W, C = img.shape\n",
        "\n",
        "        blur_map = apply_gaussian_blur(img, kernel_size, sigmaX)\n",
        "        blur_map = HWC3(blur_map)\n",
        "\n",
        "        control = torch.from_numpy(blur_map[...,::-1].copy().transpose([2,0,1])).float().to(device)[None] / 127.5 - 1\n",
        "        _ = control_lora(control).control_states\n",
        "\n",
        "        if seed == -1:\n",
        "            seed = random.randint(0, 65535)\n",
        "\n",
        "        # run inference\n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        images = []\n",
        "        for i in range(num_samples):\n",
        "            _ = control_lora(control).control_states\n",
        "            image = pipeline(\n",
        "                prompt + ', ' + a_prompt, negative_prompt=n_prompt, \n",
        "                num_inference_steps=sample_steps, guidance_scale=scale, eta=eta,\n",
        "                generator=generator, height=H, width=W).images[0]\n",
        "            images.append(np.asarray(image))\n",
        "\n",
        "        results = images\n",
        "    return [blur_map] + results\n",
        "\n",
        "\n",
        "\n",
        "block = gr.Blocks().queue()\n",
        "with block:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"## Control Stable Diffusion with Blur\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_image = gr.Image(source='upload', type=\"numpy\")\n",
        "            prompt = gr.Textbox(label=\"Prompt\")\n",
        "            run_button = gr.Button(\"Run\")\n",
        "            save_button = gr.Button(\"Save\")\n",
        "            with gr.Accordion(\"Advanced options\", open=False):\n",
        "                num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
        "                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=256)\n",
        "                kernel_size = gr.Slider(label=\"Kernel size\", minimum=1, maximum=101, value=61, step=2)\n",
        "                sigmaX = gr.Slider(label=\"SigmaX\", minimum=0, maximum=100, value=10, step=0.5)\n",
        "                sample_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n",
        "                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n",
        "                seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n",
        "                eta = gr.Number(label=\"eta\", value=0.0)\n",
        "                a_prompt = gr.Textbox(label=\"Added Prompt\", value='a high-quality, detailed, and professional image')\n",
        "                n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
        "                                      value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
        "        with gr.Column():\n",
        "            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n",
        "    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX]\n",
        "    ips_save = ips + [result_gallery]\n",
        "    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n",
        "    save_button.click(fn=save, inputs=ips_save, outputs=[])\n",
        "\n",
        "\n",
        "\n",
        "block.launch(server_name='0.0.0.0', server_port= 7861)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7861\n"
          ]
        }
      ],
      "source": [
        "block.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0179ce57686942478141e238f20f561d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "038b92205454452e9fee0406fba28a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0584eff53faa4c9ea756cef809c1f0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06cdcacda0ed440dbd95df889752576a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08a272c3046a4d78867c7254edaa6a9e",
            "max": 334643276,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be22951c14034ccf9721def748f0418d",
            "value": 334643276
          }
        },
        "076e52d984094285ba1fa7bc1a6da80b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08a272c3046a4d78867c7254edaa6a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bad5b4b98c64d4b896723b73557ecca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb3b70c9cee4b9f815e88e9f055310d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f4dfbf821bd4f24a236ef4a689d6dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c4f6d4791d34ee998677ac7c632122d",
            "placeholder": "​",
            "style": "IPY_MODEL_7f116b4277744d7683f3cc1aa11cc3c2",
            "value": " 806/806 [00:00&lt;00:00, 50.6kB/s]"
          }
        },
        "101740f9a77b4298a7fa099621b688b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fbcde9e4199488bafb89314af9ecefa",
              "IPY_MODEL_22730593a2354b7b8760451d001ce799",
              "IPY_MODEL_78bc3bb5253d41cea61ed74c4dfc389c"
            ],
            "layout": "IPY_MODEL_d5aa4122b3d3436ca2a7c1c39cc073ee"
          }
        },
        "1124a834994a46809923ea7bfca5ef99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1398519124704824af1304e7cb6fa24c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13b4f517e64844aabcb7842dd42fd228": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13c6f50c9fd343f482eda0f335d4a5e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d06225f5984c3982f2450c55364572": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_511351586f974372b2f6c6c0e8df23c4",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d247c1ba0794547bcd9fcfae1b618f8",
            "value": 11
          }
        },
        "1832a76f050a413ca20521a5190e9859": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186e285d02b54d46a166196e68016370": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d045d38fa4294c8a862540ceee4d0f14",
            "max": 308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1124a834994a46809923ea7bfca5ef99",
            "value": 308
          }
        },
        "19582b9a881f41828594e4b6fc4b9596": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19af696c5ebc4826bffa4a2d1887cda5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b464bac943a4999918da4429fd18fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1783acd2a754f6199eb7a38ea5f4c79",
            "placeholder": "​",
            "style": "IPY_MODEL_9784e490fba3435ebc460cdfdc09c72b",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "1ceb28ee3b5b4274a97d4b80cc28224d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b464bac943a4999918da4429fd18fb0",
              "IPY_MODEL_f413de77aab54fa28b0e36f32431ee4d",
              "IPY_MODEL_0f4dfbf821bd4f24a236ef4a689d6dd0"
            ],
            "layout": "IPY_MODEL_25da4c5cfc1b4eaebb920ff66d760334"
          }
        },
        "1e402023ca5c4d8fa2f7567ec4265f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f6ebe644358489f9042a601d5cbce2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1832a76f050a413ca20521a5190e9859",
            "placeholder": "​",
            "style": "IPY_MODEL_bf54803dece84c9dae5534ca1b644a74",
            "value": " 541/541 [00:00&lt;00:00, 38.2kB/s]"
          }
        },
        "1fbcde9e4199488bafb89314af9ecefa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4bc916adac349f0855eee4698852842",
            "placeholder": "​",
            "style": "IPY_MODEL_a7a47b3301614b64b1175402ac755a50",
            "value": "Downloading (…)tokenizer/vocab.json: 100%"
          }
        },
        "20754b37e94b4fafacd08629f7eeedc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22730593a2354b7b8760451d001ce799": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa9e118cef25475ebd2ddd602a7d544a",
            "max": 1059962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e402023ca5c4d8fa2f7567ec4265f04",
            "value": 1059962
          }
        },
        "2465fe7c0ab74a4080eb53dee24ff08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c769537ad1e04a0ab7b99f30a6f25ee1",
            "placeholder": "​",
            "style": "IPY_MODEL_cbad2b67e8634e19935e7e1cd08e7132",
            "value": "Downloading (…)cheduler_config.json: 100%"
          }
        },
        "25da4c5cfc1b4eaebb920ff66d760334": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26209a2b0f834e569b73a90252519426": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d034faafcf2e48cf9d81b88688f87f99",
            "placeholder": "​",
            "style": "IPY_MODEL_a1d87067b5c248898a88acb646351d8d",
            "value": " 492M/492M [00:07&lt;00:00, 78.0MB/s]"
          }
        },
        "29222ad878f44b078503f5c523133901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c45d66b1a049a2b2c90ae5e5636b3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2acf111eaec243a9b909b563a8a648fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae35ff01b9f44873a9d1d5014c6889c1",
              "IPY_MODEL_bffaee3eea6d49cda76573c7f2eef3d7",
              "IPY_MODEL_93dbee95832e416ca7d6529349217d37"
            ],
            "layout": "IPY_MODEL_50c901f0f12e466d9bd5e373ce950b3b"
          }
        },
        "2c9b11386e8c4b2587a72f7d7c2908be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9eea5e03234df5b4e449f61adf2468",
            "max": 524619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e304d882fd340bc80150bdd34ca73d3",
            "value": 524619
          }
        },
        "2f01dfebdbeb40038cfa94955ce226c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7039c6c6d5964b4da4e78dcb1d67c7fa",
              "IPY_MODEL_2c9b11386e8c4b2587a72f7d7c2908be",
              "IPY_MODEL_d00149f7320144229512d878b634b374"
            ],
            "layout": "IPY_MODEL_9c8bb3d428f946108b08270badfa8a1e"
          }
        },
        "35f2f25284df472fae4c0b5f942d3e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "378befb8b73c45a98848754a589f97a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e9a0fbea4044cdad024e6d4f5edbbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c4f6d4791d34ee998677ac7c632122d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f25a9cb320e4835ad45b31f19c3ff79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fcb638bdf4f423da9e5971c8165c0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "401b536905a74447a33419b60a42579f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "404ecdac1e4a4396a161fc182f60bf5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "427c77b11642447e9e47869d425fb024": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438f7f182ab847eb90d863be7ee3d4eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f25a9cb320e4835ad45b31f19c3ff79",
            "max": 30000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6da99b1947042fb98dd567356eb7c99",
            "value": 5000
          }
        },
        "43dac5ff93094eea8568417e0ece935c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8f84b4a31fd4a278576996b35ce554c",
              "IPY_MODEL_e47008f6e3f04cee91176c149432ced1",
              "IPY_MODEL_6c55328b46874580901c8f4da02b833b"
            ],
            "layout": "IPY_MODEL_d754221f7b424f1da317c24ed29e6e94"
          }
        },
        "43f013046c884c93b5e34d0a5adb1938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46052f71dac44e238aaba9106b6c72e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae72a057a76741fb856574c150816d47",
              "IPY_MODEL_06cdcacda0ed440dbd95df889752576a",
              "IPY_MODEL_ff2cd42d9558444aa16a6e3e74b9e6b0"
            ],
            "layout": "IPY_MODEL_e8ce7be799de41f4a296d84659b8a938"
          }
        },
        "4cc5a68581024c30a4aa61376f5e5180": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e44a695a19794f74b7c943968e460388",
              "IPY_MODEL_d0d1b5fe7c234d2faa122d807b1b3e38",
              "IPY_MODEL_cc975b6d922b4c78871a9a6a7dbf17cc"
            ],
            "layout": "IPY_MODEL_639b5747bb11424fb902b64b6b5fa1f9"
          }
        },
        "4f969668494d4f9e8b7b8f896740b719": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ebf44208a4d4607a0a5f4b4be821bed",
            "max": 3438167540,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39e9a0fbea4044cdad024e6d4f5edbbc",
            "value": 230686720
          }
        },
        "50c901f0f12e466d9bd5e373ce950b3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511351586f974372b2f6c6c0e8df23c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520af4d76b8d4e6488dc3ccca3d4a1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83d70d17163f42bfb05f23538af4d1ee",
              "IPY_MODEL_4f969668494d4f9e8b7b8f896740b719",
              "IPY_MODEL_9cbf2fb66927487e93bc25b37721cd9a"
            ],
            "layout": "IPY_MODEL_d14e6af6bdf54ddb91247ac5e2dfa549"
          }
        },
        "55a5e6cfc187405699e934063cee7ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5804ae1eb2a845ce9cee1e5f1b8cf795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cded7be708c49c389fb63d758a0a301": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b76db332f7224b0bb0f635772a962b7b",
              "IPY_MODEL_c984b1ca6ba0458797be54aabe94ec13",
              "IPY_MODEL_f423ccab28c04e64a7f97308f501323c"
            ],
            "layout": "IPY_MODEL_6cd3993fa94b4d0992717522a5fdc337"
          }
        },
        "5d482aac93994c3ea154ec09931c1dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5deddf6e7ec04837ba60fec2ba324821": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e9b1f2267dc468fa58a58dd218b2259": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f280ce78ebf413a889d868e3f6aa54b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "639b5747bb11424fb902b64b6b5fa1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6445ba572801423c9504d73d264bfd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19582b9a881f41828594e4b6fc4b9596",
            "placeholder": "​",
            "style": "IPY_MODEL_98e0e284046641deaa3ef4534f7e783a",
            "value": " 11/11 [00:00&lt;00:00,  1.98it/s]"
          }
        },
        "65df24c7d83f4b6792fb39aa6d844c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "678c8326ec4e4e8e87a2aabc4530c220": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c55328b46874580901c8f4da02b833b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5deddf6e7ec04837ba60fec2ba324821",
            "placeholder": "​",
            "style": "IPY_MODEL_c32b04b7a881493bb9a15885949c9f8c",
            "value": " 743/743 [00:00&lt;00:00, 12.4kB/s]"
          }
        },
        "6cd3993fa94b4d0992717522a5fdc337": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d247c1ba0794547bcd9fcfae1b618f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d4fb39ad1ab4393bc7d364104c06787": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7039c6c6d5964b4da4e78dcb1d67c7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d482aac93994c3ea154ec09931c1dd1",
            "placeholder": "​",
            "style": "IPY_MODEL_0bb3b70c9cee4b9f815e88e9f055310d",
            "value": "Downloading (…)tokenizer/merges.txt: 100%"
          }
        },
        "730cd649635745548aa13f347440c967": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b872d111a4134886b3039b7b0cc48f98",
            "placeholder": "​",
            "style": "IPY_MODEL_c1b2c68968ba4d1982af643c00e652a4",
            "value": " 5000/30000 [44:38&lt;3:26:58,  2.01it/s, lr=0.0001, step_loss=0.00221]"
          }
        },
        "7343b4f18cdf4b76aa1a941f6ab9caa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_853ee94ef8234dd1a4de51f4b5f244b0",
              "IPY_MODEL_811ebb183ae640d4a009a7894fc7250b",
              "IPY_MODEL_1f6ebe644358489f9042a601d5cbce2a"
            ],
            "layout": "IPY_MODEL_c83d5c1515d441c79c7e512735124eb3"
          }
        },
        "754c77edced14566a73bbe36e38a4f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "759430d292bf4e47954abc0809be3448": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7762f808c01b4c6b8a345fda6cf640a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d37dbd1e074d818197d94e01e135bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78bc3bb5253d41cea61ed74c4dfc389c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d611a42154a7467897d136fdec9dab5b",
            "placeholder": "​",
            "style": "IPY_MODEL_0179ce57686942478141e238f20f561d",
            "value": " 1.06M/1.06M [00:00&lt;00:00, 13.8MB/s]"
          }
        },
        "79220c70b8d04b9a9e17f5e84bc4868b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ba1c9ef340a47559ed70007723d0655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d1337e958eb4ed4888055a7d1d1705e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f116b4277744d7683f3cc1aa11cc3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fd37bf306d54f8eb7079aed5a97f070": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "811ebb183ae640d4a009a7894fc7250b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b545febebee44dd49c96ee303c02529e",
            "max": 541,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb0d8321e4e74731b2708009939c3d9e",
            "value": 541
          }
        },
        "83d70d17163f42bfb05f23538af4d1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6f3b83a4af34acca88be165acdff131",
            "placeholder": "​",
            "style": "IPY_MODEL_65df24c7d83f4b6792fb39aa6d844c02",
            "value": "Downloading (…)ch_model.safetensors:   7%"
          }
        },
        "853ee94ef8234dd1a4de51f4b5f244b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b884e3d925244864ba21221ffe01a0f0",
            "placeholder": "​",
            "style": "IPY_MODEL_13b4f517e64844aabcb7842dd42fd228",
            "value": "Downloading (…)ain/model_index.json: 100%"
          }
        },
        "8bcf5a68d29344c381d671bdc2abc970": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e304d882fd340bc80150bdd34ca73d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ebf44208a4d4607a0a5f4b4be821bed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93dbee95832e416ca7d6529349217d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fd37bf306d54f8eb7079aed5a97f070",
            "placeholder": "​",
            "style": "IPY_MODEL_98b76fc606ee4340808f0ce5f9a6489f",
            "value": " 617/617 [00:00&lt;00:00, 32.9kB/s]"
          }
        },
        "94e676b502294be49e0e1d6aa8a7cbae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c610bc21a18347d3b1ad203ea5836404",
              "IPY_MODEL_438f7f182ab847eb90d863be7ee3d4eb",
              "IPY_MODEL_730cd649635745548aa13f347440c967"
            ],
            "layout": "IPY_MODEL_9c51968607594b7da90149c8c2f70501"
          }
        },
        "960ef00923314b44b48dbf0f93e55d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2465fe7c0ab74a4080eb53dee24ff08f",
              "IPY_MODEL_186e285d02b54d46a166196e68016370",
              "IPY_MODEL_ad9f5a1961c74194a96fc2d45bae169e"
            ],
            "layout": "IPY_MODEL_e153811fe24248ba86e7eab04d5b321c"
          }
        },
        "9784e490fba3435ebc460cdfdc09c72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98b76fc606ee4340808f0ce5f9a6489f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98e0e284046641deaa3ef4534f7e783a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "995370a5f3bf4ba1aa4a9b8abbe1836b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab900241335744eeb1bd0dbfea25e6dc",
            "max": 492265874,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_678c8326ec4e4e8e87a2aabc4530c220",
            "value": 492265874
          }
        },
        "9b5b6f1d5af84265b2454d8e9cfbcb46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c51968607594b7da90149c8c2f70501": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c8bb3d428f946108b08270badfa8a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cbf2fb66927487e93bc25b37721cd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_378befb8b73c45a98848754a589f97a4",
            "placeholder": "​",
            "style": "IPY_MODEL_076e52d984094285ba1fa7bc1a6da80b",
            "value": " 231M/3.44G [00:03&lt;00:40, 78.3MB/s]"
          }
        },
        "9d49105631c04df3a3adbc9876c15a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1398519124704824af1304e7cb6fa24c",
            "max": 472,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35f2f25284df472fae4c0b5f942d3e11",
            "value": 472
          }
        },
        "a195f39609cf4fbc8cd43325f5db9e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7961fc269e642b68399ce2aacc3eb6e",
              "IPY_MODEL_995370a5f3bf4ba1aa4a9b8abbe1836b",
              "IPY_MODEL_26209a2b0f834e569b73a90252519426"
            ],
            "layout": "IPY_MODEL_ef2b1e23571f4ff282935a444df76305"
          }
        },
        "a1d87067b5c248898a88acb646351d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7a47b3301614b64b1175402ac755a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa9e118cef25475ebd2ddd602a7d544a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab900241335744eeb1bd0dbfea25e6dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad9f5a1961c74194a96fc2d45bae169e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bad5b4b98c64d4b896723b73557ecca",
            "placeholder": "​",
            "style": "IPY_MODEL_dfbea5904be04ca193ab9a4dc185d7b0",
            "value": " 308/308 [00:00&lt;00:00, 14.7kB/s]"
          }
        },
        "ae35ff01b9f44873a9d1d5014c6889c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_401b536905a74447a33419b60a42579f",
            "placeholder": "​",
            "style": "IPY_MODEL_0584eff53faa4c9ea756cef809c1f0a3",
            "value": "Downloading (…)_encoder/config.json: 100%"
          }
        },
        "ae6009824cec4d85ad1f3ab93bbd88aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae72a057a76741fb856574c150816d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_038b92205454452e9fee0406fba28a7b",
            "placeholder": "​",
            "style": "IPY_MODEL_7ba1c9ef340a47559ed70007723d0655",
            "value": "Downloading (…)ch_model.safetensors: 100%"
          }
        },
        "b545febebee44dd49c96ee303c02529e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6c835d2d19540378d038cdd998bcccd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6da99b1947042fb98dd567356eb7c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b76db332f7224b0bb0f635772a962b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e9b1f2267dc468fa58a58dd218b2259",
            "placeholder": "​",
            "style": "IPY_MODEL_ec64916670654434adbc0c7aa18cc72c",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "b872d111a4134886b3039b7b0cc48f98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b884e3d925244864ba21221ffe01a0f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8f84b4a31fd4a278576996b35ce554c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b5b6f1d5af84265b2454d8e9cfbcb46",
            "placeholder": "​",
            "style": "IPY_MODEL_20754b37e94b4fafacd08629f7eeedc6",
            "value": "Downloading (…)ain/unet/config.json: 100%"
          }
        },
        "b947856c025d498e86b529a57ba71d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f22b08607e294deeb886f8585366aba5",
              "IPY_MODEL_9d49105631c04df3a3adbc9876c15a5d",
              "IPY_MODEL_cf125cb518b84435996807786d06fbeb"
            ],
            "layout": "IPY_MODEL_7d1337e958eb4ed4888055a7d1d1705e"
          }
        },
        "bb528303518c420c95d7a482f7de81b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcf51503c1b24d90bb0a55741be97b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf5fa255db2142be995a7d1341aa856d",
              "IPY_MODEL_17d06225f5984c3982f2450c55364572",
              "IPY_MODEL_6445ba572801423c9504d73d264bfd1a"
            ],
            "layout": "IPY_MODEL_77d37dbd1e074d818197d94e01e135bb"
          }
        },
        "be22951c14034ccf9721def748f0418d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be767ce1c0b0435db7402b6667526565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf54803dece84c9dae5534ca1b644a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf5fa255db2142be995a7d1341aa856d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f013046c884c93b5e34d0a5adb1938",
            "placeholder": "​",
            "style": "IPY_MODEL_3fcb638bdf4f423da9e5971c8165c0ee",
            "value": "Fetching 11 files: 100%"
          }
        },
        "bffaee3eea6d49cda76573c7f2eef3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6c835d2d19540378d038cdd998bcccd",
            "max": 617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be767ce1c0b0435db7402b6667526565",
            "value": 617
          }
        },
        "c1b2c68968ba4d1982af643c00e652a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c32b04b7a881493bb9a15885949c9f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c610bc21a18347d3b1ad203ea5836404": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4fb39ad1ab4393bc7d364104c06787",
            "placeholder": "​",
            "style": "IPY_MODEL_5804ae1eb2a845ce9cee1e5f1b8cf795",
            "value": "Steps:  17%"
          }
        },
        "c769537ad1e04a0ab7b99f30a6f25ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83d5c1515d441c79c7e512735124eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c984b1ca6ba0458797be54aabe94ec13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb833fe923e94f49bd94c70c59bacdcc",
            "max": 342,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79220c70b8d04b9a9e17f5e84bc4868b",
            "value": 342
          }
        },
        "cb0d8321e4e74731b2708009939c3d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbad2b67e8634e19935e7e1cd08e7132": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc975b6d922b4c78871a9a6a7dbf17cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c6f50c9fd343f482eda0f335d4a5e2",
            "placeholder": "​",
            "style": "IPY_MODEL_df5128cdfe954ddca98544cf8f909d66",
            "value": " 547/547 [00:00&lt;00:00, 6.52kB/s]"
          }
        },
        "cf125cb518b84435996807786d06fbeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7762f808c01b4c6b8a345fda6cf640a7",
            "placeholder": "​",
            "style": "IPY_MODEL_19af696c5ebc4826bffa4a2d1887cda5",
            "value": " 472/472 [00:00&lt;00:00, 28.4kB/s]"
          }
        },
        "d00149f7320144229512d878b634b374": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29c45d66b1a049a2b2c90ae5e5636b3b",
            "placeholder": "​",
            "style": "IPY_MODEL_bb528303518c420c95d7a482f7de81b8",
            "value": " 525k/525k [00:00&lt;00:00, 10.7MB/s]"
          }
        },
        "d034faafcf2e48cf9d81b88688f87f99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d045d38fa4294c8a862540ceee4d0f14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d1b5fe7c234d2faa122d807b1b3e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_427c77b11642447e9e47869d425fb024",
            "max": 547,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f280ce78ebf413a889d868e3f6aa54b",
            "value": 547
          }
        },
        "d14e6af6bdf54ddb91247ac5e2dfa549": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19102930790402f8ed902ab2f5db661": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5aa4122b3d3436ca2a7c1c39cc073ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d611a42154a7467897d136fdec9dab5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d63a58545afa48a1b3e526ff5a28db59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d754221f7b424f1da317c24ed29e6e94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9eea5e03234df5b4e449f61adf2468": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df178eee77c5401ebdba2caa73f3a458": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df5128cdfe954ddca98544cf8f909d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df61a047c3ae4548b519556bc95a2b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfbea5904be04ca193ab9a4dc185d7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e05b5e1dec334bd5853338e53919384e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e153811fe24248ba86e7eab04d5b321c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e44a695a19794f74b7c943968e460388": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df178eee77c5401ebdba2caa73f3a458",
            "placeholder": "​",
            "style": "IPY_MODEL_55a5e6cfc187405699e934063cee7ff1",
            "value": "Downloading (…)main/vae/config.json: 100%"
          }
        },
        "e47008f6e3f04cee91176c149432ced1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e59673d05ce14c3b8965059f82757438",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae6009824cec4d85ad1f3ab93bbd88aa",
            "value": 743
          }
        },
        "e4bc916adac349f0855eee4698852842": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59673d05ce14c3b8965059f82757438": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f3b83a4af34acca88be165acdff131": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7961fc269e642b68399ce2aacc3eb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63a58545afa48a1b3e526ff5a28db59",
            "placeholder": "​",
            "style": "IPY_MODEL_404ecdac1e4a4396a161fc182f60bf5c",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "e8ce7be799de41f4a296d84659b8a938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec64916670654434adbc0c7aa18cc72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef2b1e23571f4ff282935a444df76305": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1783acd2a754f6199eb7a38ea5f4c79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22b08607e294deeb886f8585366aba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_754c77edced14566a73bbe36e38a4f6f",
            "placeholder": "​",
            "style": "IPY_MODEL_f27e05f702864c6eb29cfa8f4c00e94e",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "f27e05f702864c6eb29cfa8f4c00e94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f413de77aab54fa28b0e36f32431ee4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05b5e1dec334bd5853338e53919384e",
            "max": 806,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_759430d292bf4e47954abc0809be3448",
            "value": 806
          }
        },
        "f423ccab28c04e64a7f97308f501323c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29222ad878f44b078503f5c523133901",
            "placeholder": "​",
            "style": "IPY_MODEL_d19102930790402f8ed902ab2f5db661",
            "value": " 342/342 [00:00&lt;00:00, 13.9kB/s]"
          }
        },
        "fb833fe923e94f49bd94c70c59bacdcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2cd42d9558444aa16a6e3e74b9e6b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bcf5a68d29344c381d671bdc2abc970",
            "placeholder": "​",
            "style": "IPY_MODEL_df61a047c3ae4548b519556bc95a2b07",
            "value": " 335M/335M [00:04&lt;00:00, 63.7MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
