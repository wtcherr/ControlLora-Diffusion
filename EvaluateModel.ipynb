{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' !pip install torch>=1.6\\n!pip install torchvision\\n!pip install numpy\\n!pip install pandas\\n!pip install tqdm\\n!pip install tensorboardX>=1.14\\n!pip install scipy \\n!pip install opencv-python\\n!pip install clean-fid '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" !pip install torch>=1.6\n",
    "!pip install torchvision\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install tensorboardX>=1.14\n",
    "!pip install scipy \n",
    "!pip install opencv-python\n",
    "!pip install clean-fid \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"sd-unsplash_5k_blur_61KS-model-control-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def gather_images(model_name):\n",
    "    # create folders to store the images, or clear them if they already exist\n",
    "    for folder in ['original_images', 'generated_images']:\n",
    "        if os.path.exists(f\"{model_name}/{folder}\"):\n",
    "            # remove existing files in the folder\n",
    "            for filename in os.listdir(f\"{model_name}/{folder}\"):\n",
    "                file_path = os.path.join(f\"{model_name}/{folder}\", filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                        os.unlink(file_path)\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        else:\n",
    "            os.makedirs(f\"{model_name}/{folder}\")\n",
    "\n",
    "    # walk through the folders and subfolders to access the images\n",
    "    for dirpath, dirnames, filenames in os.walk(model_name):\n",
    "        if 'saves' in dirpath:\n",
    "            for filename in filenames:\n",
    "                if 'original_image' in filename:\n",
    "                    # get the datetime directory name\n",
    "                    datetime_dir = os.path.basename(dirpath)\n",
    "                    # copy original image to original_images folder and include datetime in name\n",
    "                    shutil.copy(os.path.join(dirpath, filename), os.path.join(f\"{model_name}/original_images\", f\"{datetime_dir}_{filename}\"))\n",
    "                elif 'generated_image' in filename:\n",
    "                    # get the datetime directory name\n",
    "                    datetime_dir = os.path.basename(dirpath)\n",
    "                    # copy generated image to generated_images folder and include datetime in name\n",
    "                    shutil.copy(os.path.join(dirpath, filename), os.path.join(f\"{model_name}/generated_images\", f\"{datetime_dir}_{filename}\"))\n",
    "\n",
    "# usage\n",
    "gather_images(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def make_dataset(dir):\n",
    "    if os.path.isfile(dir):\n",
    "        images = [i for i in np.genfromtxt(dir, dtype=np.str, encoding='utf-8')]\n",
    "    else:\n",
    "        images = []\n",
    "        assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "        for root, _, fnames in sorted(os.walk(dir)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    images.append(path)\n",
    "\n",
    "    return images\n",
    "\n",
    "def pil_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class BaseDataset(data.Dataset):\n",
    "    def __init__(self, data_root, image_size=[256, 256], loader=pil_loader):\n",
    "        self.imgs = make_dataset(data_root)\n",
    "        self.tfs = transforms.Compose([\n",
    "                transforms.Resize((image_size[0], image_size[1])),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.imgs[index]\n",
    "        img = self.tfs(self.loader(path))\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def mae(input, target):\n",
    "    with torch.no_grad():\n",
    "        loss = nn.L1Loss()\n",
    "        output = loss(input, target)\n",
    "    return output\n",
    "\n",
    "\n",
    "def inception_score(imgs, cuda=True, batch_size=32, resize=False, splits=1):\n",
    "    \"\"\"Computes the inception score of the generated images imgs\n",
    "\n",
    "    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n",
    "    cuda -- whether or not to run on GPU\n",
    "    batch_size -- batch size for feeding into Inception v3\n",
    "    splits -- number of splits\n",
    "    \"\"\"\n",
    "    N = len(imgs)\n",
    "    print(N)\n",
    "\n",
    "    assert batch_size > 0\n",
    "    assert N >= batch_size\n",
    "\n",
    "    # Set up dtype\n",
    "    if cuda:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Set up dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "\n",
    "    # Load inception model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n",
    "    inception_model.eval()\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x, dim=1).data.cpu().numpy()\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        batch = batch.type(dtype)\n",
    "        batchv = Variable(batch)\n",
    "        batch_size_i = batch.size()[0]\n",
    "\n",
    "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "    # Now compute the mean kl-div\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 14 images in the folder sd-unsplash_5k_blur_61KS-model-control-lora/original_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID original_images :   0%|          | 0/1 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m dst\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/generated_images\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#@param {type:\"string\"}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m fid_score\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m fid_score \u001b[39m=\u001b[39m fid\u001b[39m.\u001b[39;49mcompute_fid(src, dst, num_workers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m is_mean, is_std \u001b[39m=\u001b[39m inception_score(BaseDataset(dst), cuda\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, resize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, splits\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFID: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(fid_score))\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\cleanfid\\fid.py:478\u001b[0m, in \u001b[0;36mcompute_fid\u001b[1;34m(fdir1, fdir2, gen, mode, model_name, num_workers, batch_size, device, dataset_name, dataset_res, dataset_split, num_gen, z_dim, custom_feat_extractor, verbose, custom_image_tranform, custom_fn_resize, use_dataparallel)\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m    477\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcompute FID between two folders\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 478\u001b[0m     score \u001b[39m=\u001b[39m compare_folders(fdir1, fdir2, feat_model,\n\u001b[0;32m    479\u001b[0m         mode\u001b[39m=\u001b[39;49mmode, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    480\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers, device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    481\u001b[0m         custom_image_tranform\u001b[39m=\u001b[39;49mcustom_image_tranform,\n\u001b[0;32m    482\u001b[0m         custom_fn_resize\u001b[39m=\u001b[39;49mcustom_fn_resize,\n\u001b[0;32m    483\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m    484\u001b[0m     \u001b[39mreturn\u001b[39;00m score\n\u001b[0;32m    486\u001b[0m \u001b[39m# compute fid of a folder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\cleanfid\\fid.py:269\u001b[0m, in \u001b[0;36mcompare_folders\u001b[1;34m(fdir1, fdir2, feat_model, mode, num_workers, batch_size, device, verbose, custom_image_tranform, custom_fn_resize)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompare_folders\u001b[39m(fdir1, fdir2, feat_model, mode, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m    265\u001b[0m                     batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m), verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m                     custom_image_tranform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, custom_fn_resize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    267\u001b[0m     \u001b[39m# get all inception features for the first folder\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     fbname1 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(fdir1)\n\u001b[1;32m--> 269\u001b[0m     np_feats1 \u001b[39m=\u001b[39m get_folder_features(fdir1, feat_model, num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    270\u001b[0m                                     batch_size\u001b[39m=\u001b[39;49mbatch_size, device\u001b[39m=\u001b[39;49mdevice, mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    271\u001b[0m                                     description\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFID \u001b[39;49m\u001b[39m{\u001b[39;49;00mfbname1\u001b[39m}\u001b[39;49;00m\u001b[39m : \u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    272\u001b[0m                                     custom_image_tranform\u001b[39m=\u001b[39;49mcustom_image_tranform,\n\u001b[0;32m    273\u001b[0m                                     custom_fn_resize\u001b[39m=\u001b[39;49mcustom_fn_resize)\n\u001b[0;32m    274\u001b[0m     mu1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(np_feats1, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    275\u001b[0m     sigma1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(np_feats1, rowvar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\cleanfid\\fid.py:147\u001b[0m, in \u001b[0;36mget_folder_features\u001b[1;34m(fdir, model, num_workers, num, shuffle, seed, batch_size, device, mode, custom_fn_resize, description, verbose, custom_image_tranform)\u001b[0m\n\u001b[0;32m    145\u001b[0m         random\u001b[39m.\u001b[39mshuffle(files)\n\u001b[0;32m    146\u001b[0m     files \u001b[39m=\u001b[39m files[:num]\n\u001b[1;32m--> 147\u001b[0m np_feats \u001b[39m=\u001b[39m get_files_features(files, model, num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m    148\u001b[0m                               batch_size\u001b[39m=\u001b[39;49mbatch_size, device\u001b[39m=\u001b[39;49mdevice, mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    149\u001b[0m                               custom_fn_resize\u001b[39m=\u001b[39;49mcustom_fn_resize,\n\u001b[0;32m    150\u001b[0m                               custom_image_tranform\u001b[39m=\u001b[39;49mcustom_image_tranform,\n\u001b[0;32m    151\u001b[0m                               description\u001b[39m=\u001b[39;49mdescription, fdir\u001b[39m=\u001b[39;49mfdir, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m    152\u001b[0m \u001b[39mreturn\u001b[39;00m np_feats\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\cleanfid\\fid.py:118\u001b[0m, in \u001b[0;36mget_files_features\u001b[1;34m(l_files, model, num_workers, batch_size, device, mode, custom_fn_resize, description, fdir, verbose, custom_image_tranform)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     pbar \u001b[39m=\u001b[39m dataloader\n\u001b[1;32m--> 118\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m pbar:\n\u001b[0;32m    119\u001b[0m     l_feats\u001b[39m.\u001b[39mappend(get_batch_features(batch, model, device))\n\u001b[0;32m    120\u001b[0m np_feats \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(l_feats)\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\cleanfid\\utils.py:44\u001b[0m, in \u001b[0;36mResizeDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     42\u001b[0m     img_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(path)\n\u001b[0;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     img_pil \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(path)\u001b[39m.\u001b[39;49mconvert(\u001b[39m'\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     45\u001b[0m     img_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(img_pil)\n\u001b[0;32m     47\u001b[0m \u001b[39m# apply a custom image transform before resizing the image to 299x299\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Khaled\\anaconda3\\envs\\myenv\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from cleanfid import fid\n",
    "\n",
    "src=f\"{model_name}/original_images\" #@param {type:\"string\"}\n",
    "dst=f\"{model_name}/generated_images\" #@param {type:\"string\"}\n",
    "\n",
    "fid_score=0\n",
    "fid_score = fid.compute_fid(src, dst, num_workers=0)\n",
    "is_mean, is_std = inception_score(BaseDataset(dst), cuda=True, batch_size=1, resize=True, splits=1)\n",
    "\n",
    "print('FID: {}'.format(fid_score))\n",
    "print('IS:{} {}'.format(is_mean, is_std))\n",
    "\n",
    "with open(f\"{model_name}/scores.txt\", 'w') as f:\n",
    "\t# write the scores to the file\n",
    "\tf.write(f\"FID score: {fid_score}\\n\")\n",
    "\tf.write(f\"Inception score mean: {is_mean}\\n\")\n",
    "\tf.write(f\"Inception score std: {is_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VGGPerceptualLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m ToTensor\n\u001b[0;32m      4\u001b[0m \u001b[39m# Create an instance of the perceptual loss function\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m perceptual_loss \u001b[39m=\u001b[39m VGGPerceptualLoss(loss_func\u001b[39m=\u001b[39mEuclideanDistanceMean())\n\u001b[0;32m      8\u001b[0m \u001b[39m# Load your images\u001b[39;00m\n\u001b[0;32m      9\u001b[0m image1 \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39m/content/original_image.png\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VGGPerceptualLoss' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Create an instance of the perceptual loss function\n",
    "perceptual_loss = VGGPerceptualLoss(loss_func=EuclideanDistanceMean())\n",
    "\n",
    "\n",
    "# Load your images\n",
    "image1 = Image.open(\"/content/original_image.png\")\n",
    "image2 = Image.open(\"/content/generated_image_2.png\")\n",
    "\n",
    "# Transform your images to tensors\n",
    "transform = ToTensor()\n",
    "image1 = transform(image1).unsqueeze(0)  # Unsqueeze to add artificial first dimension\n",
    "image2 = transform(image2).unsqueeze(0)  # Unsqueeze to add artificial first dimension\n",
    "\n",
    "# Compute the perceptual loss\n",
    "loss = perceptual_loss(image1, image2)\n",
    "\n",
    "print(\"Perceptual loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, resize=True,loss_func=nn.MSELoss()):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        self.loss_func = loss_func\n",
    "        blocks = []\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n",
    "        for bl in blocks:\n",
    "            for p in bl:\n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.mean = torch.nn.Parameter(torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.std = torch.nn.Parameter(torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "        self.resize = resize\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.shape[1] != 3:\n",
    "            input = input.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        input = (input-self.mean) / self.std\n",
    "        target = (target-self.mean) / self.std\n",
    "        if self.resize:\n",
    "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = input\n",
    "        y = target\n",
    "        loss_function=self.loss_func\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            loss += loss_function(x, y)  #Can be any loss_function you choose\n",
    "        return loss/len(self.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanDistanceMean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EuclideanDistanceMean, self).__init__()\n",
    "        self.pairwise_distance = nn.PairwiseDistance()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.pairwise_distance(input, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sewar in c:\\users\\khaled\\anaconda3\\envs\\myenv\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\khaled\\anaconda3\\envs\\myenv\\lib\\site-packages (from sewar) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\khaled\\anaconda3\\envs\\myenv\\lib\\site-packages (from sewar) (1.10.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\khaled\\anaconda3\\envs\\myenv\\lib\\site-packages (from sewar) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sewar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sewar.full_ref import mse, rmse, psnr, uqi, ssim, ergas, scc, rase, sam, msssim, vifp\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def calculate_score(org,blur):\n",
    "\t# Resize images to the same dimensions\n",
    "\tblur = blur.resize(org.size)\n",
    "\n",
    "\t# Convert the images to numpy arrays\n",
    "\tblur_arr = np.array(blur)\n",
    "\torg_arr = np.array(org)\n",
    "\n",
    "\tperceptual_loss = VGGPerceptualLoss(loss_func=EuclideanDistanceMean())\n",
    "\n",
    "\ttransform = ToTensor()\n",
    "\timage1 = transform(org).unsqueeze(0)  # Unsqueeze to add artificial first dimension\n",
    "\timage2 = transform(blur).unsqueeze(0)  # Unsqueeze to add artificial first dimension\n",
    "\n",
    "\tquality_assessment = {}\n",
    "\n",
    "\t# Calculate the quality assessment metrics and store them in the dictionary\n",
    "\tquality_assessment[\"MSE\"] = mse(blur_arr, org_arr)\n",
    "\tquality_assessment[\"RMSE\"] = rmse(blur_arr, org_arr)\n",
    "\tquality_assessment[\"PSNR\"] = psnr(blur_arr, org_arr)\n",
    "\tquality_assessment[\"SSIM\"] = ssim(blur_arr, org_arr)\n",
    "\tquality_assessment[\"UQI\"] = uqi(blur_arr, org_arr)\n",
    "\tquality_assessment[\"MSSSIM\"] = msssim(blur_arr, org_arr)\n",
    "\tquality_assessment[\"ERGAS\"] = ergas(blur_arr, org_arr)\n",
    "\tquality_assessment[\"SCC\"] = scc(blur_arr, org_arr)\n",
    "\tquality_assessment[\"RASE\"] = rase(blur_arr, org_arr)\n",
    "\tquality_assessment[\"SAM\"] = sam(blur_arr, org_arr)\n",
    "\tquality_assessment[\"VIF\"] = vifp(blur_arr, org_arr)\n",
    "\tquality_assessment[\"PD\"] = perceptual_loss(image1, image2).item()\n",
    "\n",
    "\treturn quality_assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the main folder containing subfolders\n",
    "main_folder_path = \"sd_controlnet_canny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Initialize variables for average score calculation\n",
    "total_quality_assessment = {}\n",
    "total_num_subfolders = 0\n",
    "\n",
    "# Iterate over the subfolders in the main folder\n",
    "for subfolder_name in os.listdir(main_folder_path):\n",
    "    subfolder_path = os.path.join(main_folder_path, subfolder_name)\n",
    "\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Load the original image\n",
    "        original_image_path = os.path.join(subfolder_path, \"original_image.png\")\n",
    "        original_image = Image.open(original_image_path)\n",
    "        original_image = original_image.convert('RGB')\n",
    "        # Initialize variables for average score calculation per subfolder\n",
    "        subfolder_total_score = {}\n",
    "        subfolder_num_generated_images = {}\n",
    "\n",
    "        # Iterate over the generated images in the current subfolder\n",
    "        for file_name in os.listdir(subfolder_path):\n",
    "            if file_name.startswith(\"generated_image_\") and file_name.endswith(\".png\"):\n",
    "                # Load the generated image\n",
    "                generated_image_path = os.path.join(subfolder_path, file_name)\n",
    "                generated_image = Image.open(generated_image_path)\n",
    "                generated_image = generated_image.convert('RGB')\n",
    "                # Calculate the quality assessment metrics for the current generated image\n",
    "                image_quality_assessment = calculate_score(generated_image, original_image)\n",
    "\n",
    "                # Update the total quality assessment scores for the subfolder\n",
    "                for metric, value in image_quality_assessment.items():\n",
    "                    if metric not in subfolder_total_score:\n",
    "                        subfolder_total_score[metric] = value\n",
    "                        subfolder_num_generated_images[metric]=0\n",
    "                    else:\n",
    "                        if isinstance(value, tuple):\n",
    "                            subfolder_total_score[metric] = tuple(x + y for x, y in zip(subfolder_total_score[metric], value))\n",
    "                        else:\n",
    "                            subfolder_total_score[metric] += value\n",
    "\n",
    "                    subfolder_num_generated_images[metric] += 1\n",
    "\n",
    "        # Calculate the average scores for the current subfolder\n",
    "        subfolder_average_score = {}\n",
    "        for metric, value in subfolder_total_score.items():\n",
    "            if metric not in subfolder_average_score:\n",
    "                subfolder_average_score[metric]=value\n",
    "            else:\n",
    "                if isinstance(value, tuple):\n",
    "                    subfolder_average_score[metric] = tuple(x / subfolder_num_generated_images[metric] for x in value)\n",
    "                else:\n",
    "                    subfolder_average_score[metric] = value / subfolder_num_generated_images[metric]\n",
    "\n",
    "        # Update the total quality assessment scores across all subfolders\n",
    "        for metric, value in subfolder_average_score.items():\n",
    "            if metric not in total_quality_assessment:\n",
    "                total_quality_assessment[metric]=value\n",
    "            else:\n",
    "                if isinstance(value, tuple):\n",
    "                    total_quality_assessment[metric] = tuple(x + value[i] for i, x in enumerate(total_quality_assessment[metric]))\n",
    "                else:\n",
    "                    total_quality_assessment[metric] += value\n",
    "\n",
    "        # Increment the count of subfolders\n",
    "        total_num_subfolders += 1\n",
    "\n",
    "        # Save the quality assessment dictionary in a file within the current subfolder\n",
    "        quality_assessment_file_path = os.path.join(subfolder_path, \"quality_assessment.txt\")\n",
    "        with open(quality_assessment_file_path, \"w\") as qa_file:\n",
    "            for metric, value in subfolder_average_score.items():\n",
    "                qa_file.write(\"{}: {}\\n\".format(metric, value))\n",
    "\n",
    "# Calculate the average quality assessment scores across all subfolders\n",
    "average_quality_assessment = {}\n",
    "for metric, value in total_quality_assessment.items():\n",
    "    if isinstance(value, tuple):\n",
    "        average_quality_assessment[metric] = tuple(x / total_num_subfolders for x in value)\n",
    "    else:\n",
    "        average_quality_assessment[metric] = value / total_num_subfolders\n",
    "\n",
    "# Save the average quality assessment dictionary in model_quality_assessment.txt in the main folder\n",
    "model_quality_assessment_file_path = os.path.join(main_folder_path, \"model_quality_assessment.txt\")\n",
    "with open(model_quality_assessment_file_path, \"w\") as model_qa_file:\n",
    "    for metric, value in average_quality_assessment.items():\n",
    "        model_qa_file.write(\"{}: {}\\n\".format(metric, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_canny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
