{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\diffusers\\models\\cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "  deprecate(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from diffusers.utils.outputs import BaseOutput\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unet_2d_blocks import get_down_block as get_down_block_default\n",
    "from diffusers.models.resnet import Mish, Upsample2D, Downsample2D, upsample_2d, downsample_2d, partial\n",
    "from diffusers.models.cross_attention import CrossAttention, LoRALinearLayer # , LoRACrossAttnProcessor\n",
    "\n",
    "\n",
    "def get_down_block(\n",
    "    down_block_type,\n",
    "    num_layers,\n",
    "    in_channels,\n",
    "    out_channels,\n",
    "    temb_channels,\n",
    "    add_downsample,\n",
    "    resnet_eps,\n",
    "    resnet_act_fn,\n",
    "    attn_num_head_channels,\n",
    "    resnet_groups=None,\n",
    "    cross_attention_dim=None,\n",
    "    downsample_padding=None,\n",
    "    dual_cross_attention=False,\n",
    "    use_linear_projection=False,\n",
    "    only_cross_attention=False,\n",
    "    upcast_attention=False,\n",
    "    resnet_time_scale_shift=\"default\",\n",
    "    resnet_kernel_size=3,\n",
    "):\n",
    "    down_block_type = down_block_type[7:] if down_block_type.startswith(\"UNetRes\") else down_block_type\n",
    "    if down_block_type == \"SimpleDownEncoderBlock2D\":\n",
    "        return SimpleDownEncoderBlock2D(\n",
    "            num_layers=num_layers,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            add_downsample=add_downsample,\n",
    "            convnet_eps=resnet_eps,\n",
    "            convnet_act_fn=resnet_act_fn,\n",
    "            convnet_groups=resnet_groups,\n",
    "            downsample_padding=downsample_padding,\n",
    "            convnet_time_scale_shift=resnet_time_scale_shift,\n",
    "            convnet_kernel_size=resnet_kernel_size\n",
    "        )\n",
    "    else:\n",
    "        return get_down_block_default(\n",
    "            down_block_type,\n",
    "            num_layers,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            temb_channels,\n",
    "            add_downsample,\n",
    "            resnet_eps,\n",
    "            resnet_act_fn,\n",
    "            attn_num_head_channels,\n",
    "            resnet_groups=resnet_groups,\n",
    "            cross_attention_dim=cross_attention_dim,\n",
    "            downsample_padding=downsample_padding,\n",
    "            dual_cross_attention=dual_cross_attention,\n",
    "            use_linear_projection=use_linear_projection,\n",
    "            only_cross_attention=only_cross_attention,\n",
    "            upcast_attention=upcast_attention,\n",
    "            resnet_time_scale_shift=resnet_time_scale_shift,\n",
    "            # resnet_kernel_size=resnet_kernel_size\n",
    "        )\n",
    "\n",
    "\n",
    "class LoRACrossAttnProcessor(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            cross_attention_dim=None, \n",
    "            rank=4, \n",
    "            post_add=False,\n",
    "            key_states_skipped=False,\n",
    "            value_states_skipped=False,\n",
    "            output_states_skipped=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.rank = rank\n",
    "        self.post_add = post_add\n",
    "\n",
    "        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n",
    "        if not key_states_skipped:\n",
    "            self.to_k_lora = LoRALinearLayer(\n",
    "                hidden_size if post_add else (cross_attention_dim or hidden_size), hidden_size, rank)\n",
    "        if not value_states_skipped:\n",
    "            self.to_v_lora = LoRALinearLayer(\n",
    "                hidden_size if post_add else (cross_attention_dim or hidden_size), hidden_size, rank)\n",
    "        if not output_states_skipped:\n",
    "            self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank)\n",
    "\n",
    "        self.key_states_skipped: bool = key_states_skipped\n",
    "        self.value_states_skipped: bool = value_states_skipped\n",
    "        self.output_states_skipped: bool = output_states_skipped\n",
    "\n",
    "    def skip_key_states(self, is_skipped: bool = True):\n",
    "        if is_skipped == False:\n",
    "            assert hasattr(self, 'to_k_lora')\n",
    "        self.key_states_skipped = is_skipped\n",
    "\n",
    "    def skip_value_states(self, is_skipped: bool = True):\n",
    "        if is_skipped == False:\n",
    "            assert hasattr(self, 'to_q_lora')\n",
    "        self.value_states_skipped = is_skipped\n",
    "\n",
    "    def skip_output_states(self, is_skipped: bool = True):\n",
    "        if is_skipped == False:\n",
    "            assert hasattr(self, 'to_out_lora')\n",
    "        self.output_states_skipped = is_skipped\n",
    "\n",
    "    def __call__(\n",
    "        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n",
    "    ):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "\n",
    "        query = attn.to_q(hidden_states) \n",
    "        query = query + scale * self.to_q_lora(query if self.post_add else hidden_states)\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "\n",
    "        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states) \n",
    "        if not self.key_states_skipped:\n",
    "            key = key + scale * self.to_k_lora(key if self.post_add else encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "        if not self.value_states_skipped:\n",
    "            value = value + scale * self.to_v_lora(value if self.post_add else encoder_hidden_states)\n",
    "\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        out = attn.to_out[0](hidden_states)\n",
    "        if not self.output_states_skipped:\n",
    "            out = out + scale * self.to_out_lora(out if self.post_add else hidden_states)\n",
    "        hidden_states = out\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class ControlLoRACrossAttnProcessor(LoRACrossAttnProcessor):\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size, \n",
    "            cross_attention_dim=None, \n",
    "            rank=4, \n",
    "            control_rank=None, \n",
    "            post_add=False, \n",
    "            concat_hidden=False,\n",
    "            control_channels=None,\n",
    "            control_self_add=True,\n",
    "            key_states_skipped=False,\n",
    "            value_states_skipped=False,\n",
    "            output_states_skipped=False,\n",
    "            **kwargs):\n",
    "        super().__init__(\n",
    "            hidden_size, \n",
    "            cross_attention_dim, \n",
    "            rank, \n",
    "            post_add=post_add,\n",
    "            key_states_skipped=key_states_skipped,\n",
    "            value_states_skipped=value_states_skipped,\n",
    "            output_states_skipped=output_states_skipped)\n",
    "\n",
    "        control_rank = rank if control_rank is None else control_rank\n",
    "        control_channels = hidden_size if control_channels is None else control_channels\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.control_self_add = control_self_add if control_channels is None else False\n",
    "        self.control_states: torch.Tensor = None\n",
    "\n",
    "        self.to_control = LoRALinearLayer(\n",
    "            control_channels + (hidden_size if concat_hidden else 0), \n",
    "            hidden_size, \n",
    "            control_rank)\n",
    "        self.pre_loras: List[LoRACrossAttnProcessor] = []\n",
    "        self.post_loras: List[LoRACrossAttnProcessor] = []\n",
    "\n",
    "    def inject_pre_lora(self, lora_layer):\n",
    "        self.pre_loras.append(lora_layer)\n",
    "    \n",
    "    def inject_post_lora(self, lora_layer):\n",
    "        self.post_loras.append(lora_layer)\n",
    "\n",
    "    def inject_control_states(self, control_states):\n",
    "        self.control_states = control_states\n",
    "\n",
    "    def process_control_states(self, hidden_states, scale=1.0):\n",
    "        control_states = self.control_states.to(hidden_states.dtype)\n",
    "        if hidden_states.ndim == 3 and control_states.ndim == 4:\n",
    "            batch, _, height, width = control_states.shape\n",
    "            control_states = control_states.permute(0, 2, 3, 1).reshape(batch, height * width, -1)\n",
    "            self.control_states = control_states\n",
    "        _control_states = control_states\n",
    "        if self.concat_hidden:\n",
    "            b1, b2 = control_states.shape[0], hidden_states.shape[0]\n",
    "            if b1 != b2:\n",
    "                control_states = control_states[:,None].repeat(1, b2//b1, *([1]*(len(control_states.shape)-1)))\n",
    "                control_states = control_states.view(-1, *control_states.shape[2:])\n",
    "            _control_states = torch.cat([hidden_states, control_states], -1)\n",
    "        _control_states = scale * self.to_control(_control_states)\n",
    "        if self.control_self_add:\n",
    "            control_states = control_states + _control_states\n",
    "        else:\n",
    "            control_states = _control_states\n",
    "\n",
    "        return control_states\n",
    "\n",
    "    def __call__(\n",
    "        self, attn: CrossAttention, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0\n",
    "    ):\n",
    "        pre_lora: LoRACrossAttnProcessor\n",
    "        post_lora: LoRACrossAttnProcessor\n",
    "        assert self.control_states is not None\n",
    "\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "        query = attn.to_q(hidden_states)\n",
    "        for pre_lora in self.pre_loras:\n",
    "            lora_in = query if pre_lora.post_add else hidden_states\n",
    "            if isinstance(pre_lora, ControlLoRACrossAttnProcessor):\n",
    "                lora_in = lora_in + pre_lora.process_control_states(hidden_states, scale)\n",
    "            query = query + scale * pre_lora.to_q_lora(lora_in)\n",
    "        query = query + scale * self.to_q_lora((\n",
    "            query if self.post_add else hidden_states) + self.process_control_states(hidden_states, scale))\n",
    "        for post_lora in self.post_loras:\n",
    "            lora_in = query if post_lora.post_add else hidden_states\n",
    "            if isinstance(post_lora, ControlLoRACrossAttnProcessor):\n",
    "                lora_in = lora_in + post_lora.process_control_states(hidden_states, scale)\n",
    "            query = query + scale * post_lora.to_q_lora(lora_in)\n",
    "        query = attn.head_to_batch_dim(query)\n",
    "\n",
    "        encoder_hidden_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        for pre_lora in self.pre_loras:\n",
    "            if not pre_lora.key_states_skipped:\n",
    "                key = key + scale * pre_lora.to_k_lora(key if pre_lora.post_add else encoder_hidden_states)\n",
    "        if not self.key_states_skipped:\n",
    "            key = key + scale * self.to_k_lora(key if self.post_add else encoder_hidden_states)\n",
    "        for post_lora in self.post_loras:\n",
    "            if not post_lora.key_states_skipped:\n",
    "                key = key + scale * post_lora.to_k_lora(key if post_lora.post_add else encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "        for pre_lora in self.pre_loras:\n",
    "            if not pre_lora.value_states_skipped:\n",
    "                value = value + pre_lora.to_v_lora(value if pre_lora.post_add else encoder_hidden_states)\n",
    "        if not self.value_states_skipped:\n",
    "            value = value + scale * self.to_v_lora(value if self.post_add else encoder_hidden_states)\n",
    "        for post_lora in self.post_loras:\n",
    "            if not post_lora.value_states_skipped:\n",
    "                value = value + post_lora.to_v_lora(value if post_lora.post_add else encoder_hidden_states)\n",
    "\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "\n",
    "        # linear proj\n",
    "        out = attn.to_out[0](hidden_states)\n",
    "        for pre_lora in self.pre_loras:\n",
    "            if not pre_lora.output_states_skipped:\n",
    "                out = out + scale * pre_lora.to_out_lora(out if pre_lora.post_add else hidden_states)\n",
    "        out = out + scale * self.to_out_lora(out if self.post_add else hidden_states)\n",
    "        for post_lora in self.post_loras:\n",
    "            if not post_lora.output_states_skipped:\n",
    "                out = out + scale * post_lora.to_out_lora(out if post_lora.post_add else hidden_states)\n",
    "        hidden_states = out\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class ConvBlock2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        conv_kernel_size=3,\n",
    "        dropout=0.0,\n",
    "        temb_channels=512,\n",
    "        groups=32,\n",
    "        groups_out=None,\n",
    "        pre_norm=True,\n",
    "        eps=1e-6,\n",
    "        non_linearity=\"swish\",\n",
    "        time_embedding_norm=\"default\",\n",
    "        kernel=None,\n",
    "        output_scale_factor=1.0,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pre_norm = pre_norm\n",
    "        self.pre_norm = True\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.time_embedding_norm = time_embedding_norm\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.output_scale_factor = output_scale_factor\n",
    "\n",
    "        if groups_out is None:\n",
    "            groups_out = groups\n",
    "\n",
    "        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=conv_kernel_size, stride=1, padding=conv_kernel_size//2)\n",
    "\n",
    "        if temb_channels is not None:\n",
    "            if self.time_embedding_norm == \"default\":\n",
    "                time_emb_proj_out_channels = out_channels\n",
    "            elif self.time_embedding_norm == \"scale_shift\":\n",
    "                time_emb_proj_out_channels = out_channels * 2\n",
    "            else:\n",
    "                raise ValueError(f\"unknown time_embedding_norm : {self.time_embedding_norm} \")\n",
    "\n",
    "            self.time_emb_proj = torch.nn.Linear(temb_channels, time_emb_proj_out_channels)\n",
    "        else:\n",
    "            self.time_emb_proj = None\n",
    "\n",
    "        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        if non_linearity == \"swish\":\n",
    "            self.nonlinearity = lambda x: F.silu(x)\n",
    "        elif non_linearity == \"mish\":\n",
    "            self.nonlinearity = Mish()\n",
    "        elif non_linearity == \"silu\":\n",
    "            self.nonlinearity = nn.SiLU()\n",
    "\n",
    "        self.upsample = self.downsample = None\n",
    "        if self.up:\n",
    "            if kernel == \"fir\":\n",
    "                fir_kernel = (1, 3, 3, 1)\n",
    "                self.upsample = lambda x: upsample_2d(x, kernel=fir_kernel)\n",
    "            elif kernel == \"sde_vp\":\n",
    "                self.upsample = partial(F.interpolate, scale_factor=2.0, mode=\"nearest\")\n",
    "            else:\n",
    "                self.upsample = Upsample2D(in_channels, use_conv=False)\n",
    "        elif self.down:\n",
    "            if kernel == \"fir\":\n",
    "                fir_kernel = (1, 3, 3, 1)\n",
    "                self.downsample = lambda x: downsample_2d(x, kernel=fir_kernel)\n",
    "            elif kernel == \"sde_vp\":\n",
    "                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)\n",
    "            else:\n",
    "                self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name=\"op\")\n",
    "\n",
    "    def forward(self, input_tensor, temb):\n",
    "        hidden_states = input_tensor\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            # upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\n",
    "            if hidden_states.shape[0] >= 64:\n",
    "                input_tensor = input_tensor.contiguous()\n",
    "                hidden_states = hidden_states.contiguous()\n",
    "            input_tensor = self.upsample(input_tensor)\n",
    "            hidden_states = self.upsample(hidden_states)\n",
    "        elif self.downsample is not None:\n",
    "            input_tensor = self.downsample(input_tensor)\n",
    "            hidden_states = self.downsample(hidden_states)\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        if temb is not None:\n",
    "            temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
    "\n",
    "        if temb is not None and self.time_embedding_norm == \"default\":\n",
    "            hidden_states = hidden_states + temb\n",
    "\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        if temb is not None and self.time_embedding_norm == \"scale_shift\":\n",
    "            scale, shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = hidden_states * (1 + scale) + shift\n",
    "\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        output_tensor = self.dropout(hidden_states)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class SimpleDownEncoderBlock2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        dropout: float = 0.0,\n",
    "        num_layers: int = 1,\n",
    "        convnet_eps: float = 1e-6,\n",
    "        convnet_time_scale_shift: str = \"default\",\n",
    "        convnet_act_fn: str = \"swish\",\n",
    "        convnet_groups: int = 32,\n",
    "        convnet_pre_norm: bool = True,\n",
    "        convnet_kernel_size: int = 3,\n",
    "        output_scale_factor=1.0,\n",
    "        add_downsample=True,\n",
    "        downsample_padding=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        convnets = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else out_channels\n",
    "            convnets.append(\n",
    "                ConvBlock2D(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    temb_channels=None,\n",
    "                    eps=convnet_eps,\n",
    "                    groups=convnet_groups,\n",
    "                    dropout=dropout,\n",
    "                    time_embedding_norm=convnet_time_scale_shift,\n",
    "                    non_linearity=convnet_act_fn,\n",
    "                    output_scale_factor=output_scale_factor,\n",
    "                    pre_norm=convnet_pre_norm,\n",
    "                    conv_kernel_size=convnet_kernel_size,\n",
    "                )\n",
    "            )\n",
    "        in_channels = in_channels if num_layers == 0 else out_channels\n",
    "\n",
    "        self.convnets = nn.ModuleList(convnets)\n",
    "\n",
    "        if add_downsample:\n",
    "            self.downsamplers = nn.ModuleList(\n",
    "                [\n",
    "                    Downsample2D(\n",
    "                        in_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.downsamplers = None\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        for convnet in self.convnets:\n",
    "            hidden_states = convnet(hidden_states, temb=None)\n",
    "\n",
    "        if self.downsamplers is not None:\n",
    "            for downsampler in self.downsamplers:\n",
    "                hidden_states = downsampler(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ControlLoRAOutput(BaseOutput):\n",
    "    control_states: Tuple[torch.FloatTensor]\n",
    "\n",
    "\n",
    "class ControlLoRA(ModelMixin, ConfigMixin):\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        down_block_types: Tuple[str] = (\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "        ),\n",
    "        block_out_channels: Tuple[int] = (32, 64, 128, 256),\n",
    "        layers_per_block: int = 1,\n",
    "        act_fn: str = \"silu\",\n",
    "        norm_num_groups: int = 32,\n",
    "        lora_pre_down_block_types: Tuple[str] = (\n",
    "            None,\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "        ),\n",
    "        lora_pre_down_layers_per_block: int = 1,\n",
    "        lora_pre_conv_skipped: bool = False,\n",
    "        lora_pre_conv_types: Tuple[str] = (\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "            \"SimpleDownEncoderBlock2D\",\n",
    "        ),\n",
    "        lora_pre_conv_layers_per_block: int = 1,\n",
    "        lora_pre_conv_layers_kernel_size: int = 1,\n",
    "        lora_block_in_channels: Tuple[int] = (256, 256, 256, 256),\n",
    "        lora_block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n",
    "        lora_cross_attention_dims: Tuple[List[int]] = (\n",
    "            [None, 768, None, 768, None, 768, None, 768, None, 768], \n",
    "            [None, 768, None, 768, None, 768, None, 768, None, 768], \n",
    "            [None, 768, None, 768, None, 768, None, 768, None, 768], \n",
    "            [None, 768]\n",
    "        ),\n",
    "        lora_rank: int = 4,\n",
    "        lora_control_rank: int = None,\n",
    "        lora_post_add: bool = False,\n",
    "        lora_concat_hidden: bool = False,\n",
    "        lora_control_channels: Tuple[int] = (None, None, None, None),\n",
    "        lora_control_self_add: bool = True,\n",
    "        lora_key_states_skipped: bool = False,\n",
    "        lora_value_states_skipped: bool = False,\n",
    "        lora_output_states_skipped: bool = False,\n",
    "        lora_control_version: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        lora_control_cls = ControlLoRACrossAttnProcessor\n",
    "        if lora_control_version == 2:\n",
    "            lora_control_cls = ControlLoRACrossAttnProcessorV2\n",
    "\n",
    "        assert lora_block_in_channels[0] == block_out_channels[-1]\n",
    "        \n",
    "        if lora_pre_conv_skipped:\n",
    "            lora_control_channels = lora_block_in_channels\n",
    "            lora_control_self_add = False\n",
    "\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.lora_pre_down_layers_per_block = lora_pre_down_layers_per_block\n",
    "        self.lora_pre_conv_layers_per_block = lora_pre_conv_layers_per_block\n",
    "\n",
    "        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "        self.pre_lora_layers = nn.ModuleList([])\n",
    "        self.lora_layers = nn.ModuleList([])\n",
    "\n",
    "        # pre_down\n",
    "        pre_down_blocks = []\n",
    "        output_channel = block_out_channels[0]\n",
    "        for i, down_block_type in enumerate(down_block_types):\n",
    "            input_channel = output_channel\n",
    "            output_channel = block_out_channels[i]\n",
    "            is_final_block = i == len(block_out_channels) - 1\n",
    "\n",
    "            pre_down_block = get_down_block(\n",
    "                down_block_type,\n",
    "                num_layers=self.layers_per_block,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                add_downsample=not is_final_block,\n",
    "                resnet_eps=1e-6,\n",
    "                downsample_padding=0,\n",
    "                resnet_act_fn=act_fn,\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attn_num_head_channels=None,\n",
    "                temb_channels=None,\n",
    "            )\n",
    "            pre_down_blocks.append(pre_down_block)\n",
    "        self.down_blocks.append(nn.Sequential(*pre_down_blocks))\n",
    "        self.pre_lora_layers.append(\n",
    "            get_down_block(\n",
    "                lora_pre_conv_types[0],\n",
    "                num_layers=self.lora_pre_conv_layers_per_block,\n",
    "                in_channels=lora_block_in_channels[0],\n",
    "                out_channels=(\n",
    "                    lora_block_out_channels[0] \n",
    "                    if lora_control_channels[0] is None \n",
    "                    else lora_control_channels[0]),\n",
    "                add_downsample=False,\n",
    "                resnet_eps=1e-6,\n",
    "                downsample_padding=0,\n",
    "                resnet_act_fn=act_fn,\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attn_num_head_channels=None,\n",
    "                temb_channels=None,\n",
    "                resnet_kernel_size=lora_pre_conv_layers_kernel_size,\n",
    "            ) if not lora_pre_conv_skipped else nn.Identity()\n",
    "        )\n",
    "        self.lora_layers.append(\n",
    "            nn.ModuleList([\n",
    "                lora_control_cls(\n",
    "                    lora_block_out_channels[0], \n",
    "                    cross_attention_dim=cross_attention_dim, \n",
    "                    rank=lora_rank, \n",
    "                    control_rank=lora_control_rank,\n",
    "                    post_add=lora_post_add,\n",
    "                    concat_hidden=lora_concat_hidden,\n",
    "                    control_channels=lora_control_channels[0],\n",
    "                    control_self_add=lora_control_self_add,\n",
    "                    key_states_skipped=lora_key_states_skipped,\n",
    "                    value_states_skipped=lora_value_states_skipped,\n",
    "                    output_states_skipped=lora_output_states_skipped)\n",
    "                for cross_attention_dim in lora_cross_attention_dims[0]\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # down\n",
    "        output_channel = lora_block_in_channels[0]\n",
    "        for i, down_block_type in enumerate(lora_pre_down_block_types):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            input_channel = output_channel\n",
    "            output_channel = lora_block_in_channels[i]\n",
    "\n",
    "            down_block = get_down_block(\n",
    "                down_block_type,\n",
    "                num_layers=self.lora_pre_down_layers_per_block,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channel,\n",
    "                add_downsample=True,\n",
    "                resnet_eps=1e-6,\n",
    "                downsample_padding=0,\n",
    "                resnet_act_fn=act_fn,\n",
    "                resnet_groups=norm_num_groups,\n",
    "                attn_num_head_channels=None,\n",
    "                temb_channels=None,\n",
    "            )\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "            self.pre_lora_layers.append(\n",
    "                get_down_block(\n",
    "                    lora_pre_conv_types[i],\n",
    "                    num_layers=self.lora_pre_conv_layers_per_block,\n",
    "                    in_channels=output_channel,\n",
    "                    out_channels=(\n",
    "                        lora_block_out_channels[i] \n",
    "                        if lora_control_channels[i] is None \n",
    "                        else lora_control_channels[i]),\n",
    "                    add_downsample=False,\n",
    "                    resnet_eps=1e-6,\n",
    "                    downsample_padding=0,\n",
    "                    resnet_act_fn=act_fn,\n",
    "                    resnet_groups=norm_num_groups,\n",
    "                    attn_num_head_channels=None,\n",
    "                    temb_channels=None,\n",
    "                    resnet_kernel_size=lora_pre_conv_layers_kernel_size,\n",
    "                ) if not lora_pre_conv_skipped else nn.Identity()\n",
    "            )\n",
    "            self.lora_layers.append(\n",
    "                nn.ModuleList([\n",
    "                    lora_control_cls(\n",
    "                        lora_block_out_channels[i], \n",
    "                        cross_attention_dim=cross_attention_dim, \n",
    "                        rank=lora_rank, \n",
    "                        control_rank=lora_control_rank,\n",
    "                        post_add=lora_post_add,\n",
    "                        concat_hidden=lora_concat_hidden,\n",
    "                        control_channels=lora_control_channels[i],\n",
    "                        control_self_add=lora_control_self_add,\n",
    "                        key_states_skipped=lora_key_states_skipped,\n",
    "                        value_states_skipped=lora_value_states_skipped,\n",
    "                        output_states_skipped=lora_output_states_skipped)\n",
    "                    for cross_attention_dim in lora_cross_attention_dims[i]\n",
    "                ])\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, return_dict: bool = True) -> Union[ControlLoRAOutput, Tuple]:\n",
    "        lora_layer: ControlLoRACrossAttnProcessor\n",
    "        \n",
    "        orig_dtype = x.dtype\n",
    "        dtype = self.conv_in.weight.dtype\n",
    "\n",
    "        h = x.to(dtype)\n",
    "        h = self.conv_in(h)\n",
    "        control_states_list = []\n",
    "\n",
    "        # down\n",
    "        for down_block, pre_lora_layer, lora_layer_list in zip(\n",
    "            self.down_blocks, self.pre_lora_layers, self.lora_layers):\n",
    "            h = down_block(h)\n",
    "            control_states = pre_lora_layer(h)\n",
    "            if isinstance(control_states, tuple):\n",
    "                control_states = control_states[0]\n",
    "            control_states = control_states.to(orig_dtype)\n",
    "            for lora_layer in lora_layer_list:\n",
    "                lora_layer.inject_control_states(control_states)\n",
    "            control_states_list.append(control_states)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(control_states_list)\n",
    "\n",
    "        return ControlLoRAOutput(control_states=tuple(control_states_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def HWC3(x):\n",
    "    assert x.dtype == np.uint8\n",
    "    if x.ndim == 2:\n",
    "        x = x[:, :, None]\n",
    "    assert x.ndim == 3\n",
    "    H, W, C = x.shape\n",
    "    assert C == 1 or C == 3 or C == 4\n",
    "    if C == 3:\n",
    "        return x\n",
    "    if C == 1:\n",
    "        return np.concatenate([x, x, x], axis=2)\n",
    "    if C == 4:\n",
    "        color = x[:, :, 0:3].astype(np.float32)\n",
    "        alpha = x[:, :, 3:4].astype(np.float32) / 255.0\n",
    "        y = color * alpha + 255.0 * (1.0 - alpha)\n",
    "        y = y.clip(0, 255).astype(np.uint8)\n",
    "        return y\n",
    "\n",
    "\n",
    "def resize_image(input_image, resolution):\n",
    "    H, W, C = input_image.shape\n",
    "    H = float(H)\n",
    "    W = float(W)\n",
    "    k = float(resolution) / min(H, W)\n",
    "    H *= k\n",
    "    W *= k\n",
    "    H = int(np.round(H / 64.0)) * 64\n",
    "    W = int(np.round(W / 64.0)) * 64\n",
    "    img = cv2.resize(input_image, (W, H), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur:\n",
    "    def __call__(self, img, kernel_size, sigmaX):\n",
    "        return cv2.GaussianBlur(img, (kernel_size, kernel_size), sigmaX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sd-unsplash_5k_blur_61KS-model-control-lora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae\\diffusion_pytorch_model.safetensors not found\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "c:\\Users\\Khaled\\anaconda3\\envs\\lora_canny\\lib\\site-packages\\gradio\\components.py:4417: UserWarning: The 'grid' parameter will be deprecated. Please use 'columns' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7861\n",
      "Running on public URL: https://5590e8faa516f67810.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5590e8faa516f67810.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import random\n",
    "from diffusers import utils\n",
    "from diffusers.utils import deprecation_utils\n",
    "from diffusers.models import cross_attention\n",
    "utils.deprecate = lambda *arg, **kwargs: None\n",
    "deprecation_utils.deprecate = lambda *arg, **kwargs: None\n",
    "cross_attention.deprecate = lambda *arg, **kwargs: None\n",
    "\n",
    "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers.pipelines import DiffusionPipeline\n",
    "from diffusers.schedulers import DPMSolverMultistepScheduler\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "apply_gaussian_blur = GaussianBlur()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    'runwayml/stable-diffusion-v1-5', safety_checker=None\n",
    ")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline = pipeline.to(device)\n",
    "unet: UNet2DConditionModel = pipeline.unet\n",
    "\n",
    "control_lora = ControlLoRA.from_pretrained('ckpts\\sd-unsplash_5k_blur_61KS-model-control-lora')\n",
    "control_lora = control_lora.to(device)\n",
    "\n",
    "\n",
    "# load control lora attention processors\n",
    "lora_attn_procs = {}\n",
    "lora_layers_list = list([list(layer_list) for layer_list in control_lora.lora_layers])\n",
    "n_ch = len(unet.config.block_out_channels)\n",
    "control_ids = [i for i in range(n_ch)]\n",
    "for name in pipeline.unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        control_id = control_ids[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        control_id = list(reversed(control_ids))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        control_id = control_ids[block_id]\n",
    "\n",
    "    lora_layers = lora_layers_list[control_id]\n",
    "    if len(lora_layers) != 0:\n",
    "        lora_layer: ControlLoRACrossAttnProcessor = lora_layers.pop(0)\n",
    "        lora_attn_procs[name] = lora_layer\n",
    "\n",
    "unet.set_attn_processor(lora_attn_procs)\n",
    "\n",
    "\n",
    "def save(*args):\n",
    "    # unpack arguments\n",
    "    input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX, result_images = args\n",
    "\n",
    "    # check if result_images is empty\n",
    "    if not result_images:\n",
    "        return\n",
    "\n",
    "    # create save directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_folder = os.path.join(model_name, \"saves\", timestamp)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Saving parameters\n",
    "    with open(os.path.join(save_folder, \"parameters.txt\"), \"w\") as f:\n",
    "        params = [prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX]\n",
    "        param_names = [\"prompt\", \"a_prompt\", \"n_prompt\", \"num_samples\", \"image_resolution\", \"sample_steps\", \"scale\", \"seed\", \"eta\", \"kernel_size\", \"sigmaX\"]\n",
    "        for name, value in zip(param_names, params):\n",
    "            f.write(f\"{name}: {value}\\n\")\n",
    "\n",
    "    # save original image\n",
    "    input_image_pil = Image.fromarray(input_image)\n",
    "    input_image_pil.save(os.path.join(save_folder, \"original_image.png\"))\n",
    "\n",
    "    # Saving images\n",
    "    for i, result_image_dict in enumerate(result_images):\n",
    "        # get the filename of the result_image\n",
    "        result_image_filename = result_image_dict['name']\n",
    "        # read the image file as a numpy array\n",
    "        result_image_np = cv2.imread(result_image_filename)\n",
    "        # convert the numpy array to a PIL image\n",
    "        result_image_pil = Image.fromarray(cv2.cvtColor(result_image_np, cv2.COLOR_BGR2RGB))\n",
    "        if i == 0:\n",
    "            # save the guide image\n",
    "            result_image_pil.save(os.path.join(save_folder, \"guide_image.png\"))\n",
    "        else:\n",
    "            # save the generated image\n",
    "            result_image_pil.save(os.path.join(save_folder, f\"generated_image_{i}.png\"))\n",
    "\n",
    "def process(input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX):\n",
    "    with torch.no_grad():\n",
    "        img = resize_image(HWC3(input_image), image_resolution)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        blur_map = apply_gaussian_blur(img, kernel_size, sigmaX)\n",
    "        blur_map = HWC3(blur_map)\n",
    "\n",
    "        control = torch.from_numpy(blur_map[...,::-1].copy().transpose([2,0,1])).float().to(device)[None] / 127.5 - 1\n",
    "        _ = control_lora(control).control_states\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "\n",
    "        # run inference\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "        images = []\n",
    "        for i in range(num_samples):\n",
    "            _ = control_lora(control).control_states\n",
    "            image = pipeline(\n",
    "                prompt + ', ' + a_prompt, negative_prompt=n_prompt, \n",
    "                num_inference_steps=sample_steps, guidance_scale=scale, eta=eta,\n",
    "                generator=generator, height=H, width=W).images[0]\n",
    "            images.append(np.asarray(image))\n",
    "\n",
    "        results = images\n",
    "    return [blur_map] + results\n",
    "\n",
    "\n",
    "\n",
    "block = gr.Blocks().queue()\n",
    "with block:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"## Control Stable Diffusion with Blur\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_image = gr.Image(source='upload', type=\"numpy\")\n",
    "            prompt = gr.Textbox(label=\"Prompt\")\n",
    "            run_button = gr.Button(\"Run\")\n",
    "            save_button = gr.Button(\"Save\")\n",
    "            with gr.Accordion(\"Advanced options\", open=False):\n",
    "                num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
    "                image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=256)\n",
    "                kernel_size = gr.Slider(label=\"Kernel size\", minimum=1, maximum=101, value=61, step=2)\n",
    "                sigmaX = gr.Slider(label=\"SigmaX\", minimum=0, maximum=100, value=10, step=0.5)\n",
    "                sample_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=30, step=1)\n",
    "                scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=9.0, step=0.1)\n",
    "                seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=2147483647, step=1, randomize=True)\n",
    "                eta = gr.Number(label=\"eta\", value=0.0)\n",
    "                a_prompt = gr.Textbox(label=\"Added Prompt\", value='a high-quality, detailed, and professional image')\n",
    "                n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
    "                                      value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
    "        with gr.Column():\n",
    "            result_gallery = gr.Gallery(label='Output', show_label=False, elem_id=\"gallery\").style(grid=2, height='auto')\n",
    "    ips = [input_image, prompt, a_prompt, n_prompt, num_samples, image_resolution, sample_steps, scale, seed, eta, kernel_size, sigmaX]\n",
    "    ips_save = ips + [result_gallery]\n",
    "    run_button.click(fn=process, inputs=ips, outputs=[result_gallery])\n",
    "    save_button.click(fn=save, inputs=ips_save, outputs=[])\n",
    "\n",
    "\n",
    "\n",
    "block.launch(server_name='0.0.0.0', server_port= 7861, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "block.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_canny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
